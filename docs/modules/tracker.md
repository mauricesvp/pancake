# Tracker

## Centroid    
  **Functionality**

  The Centroid-Tracker is a basic but very fast and reliable tracker. It operates using the euclidean distances between centroids of objects. Further information about the Centroid-Tracker used as a basis for this work can be found [here](https://www.pyimagesearch.com/2018/07/23/simple-object-tracking-with-opencv/). Moreover, the algorithmic background presented by previous groups was adjusted and extended to fit the new concept.

  In this work the Centroid-Tracker was enhanced to allow for tracking of objects with unreliable detections and over the range of a stitched panorama generated from different camera streams. This results in special care being taken with wide camera angles due to the perspective size decrease and the transition regions between the stitched images.

  <img width="5324" height="%" src="/gitimg/Centroid_Regions.png">

  The possible tracking match cases of objects between frames was limited to their surroundings due to their limited speed. This resulted in a rectangle around each object representing an area of possible movement between frames. This rectangle was further adjusted to allow for rotations due to the non-horizontal movement of objects especially on the side cameras. This resulted in these match area rectangles to be generated by the predicted movement of objects, their last known position and some padding. The padding was added to allow for acceleration or lane swapping of the object or even frame drops in the video source.
  
  <img width="100" height="%" src="/gitimg/Centroid_Matchregions.png">

  To compensate for unreliable detections - e.g. caused by trees blocking the view or the change of view of the object due to the prespective shift - objects that are not being detected are held in memory for a certain amount of frames and are additionally actively premoved to account for their position after a possible redetection. This movement prediction is based on the past known movement of the object and thus allows for a very fast handling.
  However, due to the added time objects are stored for a possible redetection, objects that leave the recorded area are needlessly held on to. This problem was addressed by adding deregistration zones at the end of the recorded area where object are immediately removed from memory. These deregistration zones are present only in the direction of travel, so the separator line - defining the middle of the road - makes sure that these zones do not thread into the cameras incoming traffic.

  Since non-moving objects, especially partially occluded non-moving objects, resulted in unreliable detections, a registration zone was added for incoming traffic into the cameras are of view. Only during the very first frame or afterwards only inside the registration zone, objects are being added to the list of tracked centroids. This allows for a more robust tracking and less matching errors between centroids.

  At last, a transition zone was added between the different camera streams. This zone enables a better continued movement of objects in these regions which allows for higher temporal inconsistencies between camera streams, making the tracking as a whole more reliable. 
  
  **Limitations**

  The achieved results using the Centroid-Tracker are very promissing. However, objects that are bunched up in a small area, especially in the wide angles of the cameras, pose some track matching errors. Due to the dynamically adjustable match area rectanlgles these errors where reduced to a minimum, but in some special edge cases they are still present (e.g. close driving cars right after a mutual lane swap). This problem could be tackled by implementing even stricter movement prediction rules and/or taking the actual image into account; But this would most likely deminish the computational performance lead of the Centroid-Tracker.

  Furthermore, in very specific edge cases close to each other, partially occluded, parked cars combined with alternating detection leads to a track being assumed. This problem could be handled by adjusting the region of interest (ROI) to cut out the parked cars completely. However, this would require a non axis aligned ROI which makes implementation harder. 

  **Configuration options:** (under <code>CENTROID:</code>)
  
  | Parameter               | Example Values   | Description         |
  | ---------------------   | ----------------- | ------------------- |
  | <code>TRACKER_CFG_PATH</code> | "../configs/tracker/centroid.yaml"       | Centroid config path
  
      
  **centroid.yaml:**
  
  | Parameter               | Example Values   | Description         |
  | ---------------------   | ----------------- | ------------------- |
  | <code>MAX_ID</code> | 100000       | Highest possible id for tracked entities
  | <code>MAX_DISAPPEARED</code> | 10       | Maximum time (in frames) an object will be premoved after disappearance
  | <code>USE_BETTER_RECTS</code> | True       | Use rotated match rectangles based on predicted positions for better tracking
  | <code>USE_DYNAMIC_SCALING</code> | False       | Scale the match rectangles according to the perspective size decrease objects 
  | <code>DISTANCE_TOLERANCE</code> | 500       | Maximum distance to allow for an object tracking match (only active if USE_BETTER_RECTS is False)
  | <code>VERTICAL_TOLERANCE</code> | 100       | Maximum vertical distance to allow for an tracking tracking match (only active if USE_BETTER_RECTS is False)
  | <code>FRONT_DISTANCE_TOLERANCE</code> | 300       | Padding added to the direction of movement of the object for tracking matches (only active if USE_BETTER_RECTS is True)
  | <code>BACK_DISTANCE_TOLERANCE</code> | 180       | Padding added to the back of an object for tracking matches (only active if USE_BETTER_RECTS is True)
  | <code>SIDE_DISTANCE_TOLERANCE</code> | 50       | Padding added to the sides of an object to allow for tracking matches (only active if USE_BETTER_RECTS is True)
  | <code>FRAME_WIDTH</code> | 11520       | Total image width
  | <code>TRANSITION_WIDTH</code> | 200       | Transition region width between camera images
  | <code>LANE_SEPARATOR_LL</code> | 1117       | y-coordinate of the separator line (middle of the road parking area) - left
  | <code>LANE_SEPARATOR_LC</code> | 925       | y-coordinate of the separator line - left-center
  | <code>LANE_SEPARATOR_CR</code> | 925       | y-coordinate of the separator line - right-center
  | <code>LANE_SEPARATOR_CR</code> | 1151       | y-coordinate of the separator line - right
  | <code>DEREG_ZONE_L</code> | 1600       | Deregistration zone x-boundary left
  | <code>DEREG_ZONE_R</code> | 10500       | Deregistration zone x-boundary right
  | <code>REG_ZONE_L</code> | 2750       | Registration zone x-boundary left
  | <code>REG_ZONE_R</code> | 9750       | Registration zone x-boundary right

## DeepSORT
  **Functionality**

  DeepSORT is a tracking-by-detection algorithm which considers both the bounding boxes of the detection results and the information about appearance of the tracked objects. Therefore, DeepSORT is very sophisticated but also complex and slower algortihm compared to the Centroid-Tracker. Thereby making real-time tracking very hard. The algorithm is implemented as provided by [Wojke, Nicolai and Bewley, Alex](https://github.com/nwojke/deep_sort) and was only adjusted to accept a common interface between trackers.

  <img width="895" height="%" src="/gitimg/DeepSORT_Functionality.jpg">

  **Limitations**
  
  Even though DeepSORT offers very good results, also with stiched panoramic images, the feature extractor - being the main part of the "Deep" side of DeepSORT - needs further training on the objects being tracked. Especially since objects appear first as a frontal upper view, convert to a fully upper side view (in the center camera image) and exit seen from the upper back; there was no training to compensate for these disturbances.
  Furthermore, the ROI could be adjusted to completely cut out parked objectes to allow for a more reliable tracking. However, this would require a non axis aligned ROI which makes implementation harder.

  **Configuration options:** (under <code>DEEPSORT:</code>)
  
  | Parameter               | Example Values   | Description         |
  | ---------------------   | ----------------- | ------------------- |
  | <code>TRACKER_CFG_PATH</code> | "../configs/tracker/deep_sort.yaml"       | DeepSORT config path
  
      
  **deep_sort.yaml:**
  
  | Parameter               | Example Values   | Description         |
  | ---------------------   | ----------------- | ------------------- |
  | <code>REID_CKPT</code> | "../weights/tracker/deepsort/feature_extractor.t7"       | Path to the feature extractor to allow for relative addressing
  | <code>MAX_DIST</code> | 0.6       | Maximum cosine distance to nearest neighbor to allow for a tracking match
  | <code>MIN_CONFIDENCE</code> | 0.4       | Minimum confidence for an object to be considered
  | <code>NMS_MAX_OVERLAP</code> | 0.7       | Non-maximum Suppression max overlap fraction. ROIs that overlap more than this values are suppressed
  | <code>MAX_IOU_DISTANCE</code> | 0.75       | Max intersection over union distance. Associations with cost larger than this value are disregarded
  | <code>MAX_AGE</code> | 70       | Maximum number of missed timesteps before a track is deleted
  | <code>N_INIT</code> | 3       | Number of consecutive detections before the track is confirmed
  | <code>NN_BUDGET</code> | 10000       | Maximum number of samples. Removes the oldest samples when the budget is reached
  | <code>MAX_ID</code> | 100000       | Highest possible id for tracked entities
