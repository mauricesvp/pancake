<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.9.2" />
<title>pancake.models.tensorrt.yolov5_trt_2 API documentation</title>
<meta name="description" content="YOLOv5 TensorRT Class" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>pancake.models.tensorrt.yolov5_trt_2</code></h1>
</header>
<section id="section-intro">
<p>YOLOv5 TensorRT Class</p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">&#34;&#34;&#34; YOLOv5 TensorRT Class &#34;&#34;&#34;
from typing import Type, List, Union

import numpy as np
import itertools
import pkg_resources
import time

import torch
import torchvision

from pancake.logger import setup_logger
from pancake.models.base_class import BaseModel
from pancake.utils.general import (
    check_requirements,
    xywh2xyxy,
)
from pancake.utils.function_profiler import profile

l = setup_logger(__name__)

for package in [&#34;tensorrt&#34;]:
    try:
        dist = pkg_resources.get_distribution(package)
        l.info(&#34;\u2713 &#34; + &#34;{} ({}) is installed&#34;.format(dist.key, dist.version))
        import tensorrt as trt

        trt_installed = True

        check_requirements([&#34;pycuda&#34;, &#34;torchvision&#34;])
        import pycuda.driver as cuda
        import pycuda.autoinit
    except:
        l.info(&#34;\u2620 &#34; + &#34;{} is NOT installed&#34;.format(package))
        trt_installed = False


class Yolov5TRT(BaseModel):
    def __init__(
        self, yolov5, engine_path: str, plugin_path: str, device: str, *args, **kwargs
    ):
        &#34;&#34;&#34; YOLOv5 TensorRT Class

        Description:
            In order to use this class it is required to first have CUDA Toolkit, \
            CuDNN and TensorRT installed. Furthermore, you need to generate a \
            TensorRT engine and plugin library with this external repo:

            https://github.com/wang-xinyu/tensorrtx/tree/master/yolov5

        Args:
            yolov5 (YOLOCustomDetector): Instance of YOLOCustomDetector
            engine_path (str): Path of the TRT engine
            plugin_path (str): Path of the TRT plugin
            device (str): Device number

        Raises:
            ModuleNotFoundError: If TensorRT library is not installed
            ModuleNotFoundError: When invalid device index was provided
            ModuleNotFoundError: When loading of the engine and plugin failed
        &#34;&#34;&#34;
        # if trt not available return standard yolov5 model
        if not trt_installed:
            l.info(&#34;TensorRT not installed, using standard Yolov5..&#34;)
            raise ModuleNotFoundError

        # store standard model
        self._yolov5 = yolov5
        self.names = self._yolov5.names
        self._required_img_size = self._yolov5._required_img_size
        self._stride = None

        # get trt engine- and plugin path
        self._engine_path = engine_path
        self._plugin_path = plugin_path

        # create a context on this device
        try:
            device = int(device)
        except:
            l.info(
                f&#34;Given device {device} was not a device index! Using standard Yolov5..&#34;
            )
            raise ModuleNotFoundError

        self.ctx = cuda.Device(device).make_context()
        self.stream = cuda.Stream()
        TRT_LOGGER = trt.Logger(trt.Logger.INFO)
        self.runtime = trt.Runtime(TRT_LOGGER)

        # loard TRT engine
        if not self.load_engine():
            raise ModuleNotFoundError

        # allocate buffers and warm up context
        self.allocate_buffers()
        self._init_infer([self.batch_size, 3, self.input_h, self.input_w])

    def load_engine(self) -&gt; bool:
        &#34;&#34;&#34;Loads the plugin library and TRT engine.

        Returns:
            bool: success flag
        &#34;&#34;&#34;
        l.info(f&#34;Loading TRT engine from {self._engine_path}..&#34;)
        try:
            # Load trt plugin lib
            import ctypes

            ctypes.CDLL(self._plugin_path)
        except Exception as e:
            l.info(
                f&#34;Error occured while loading TRT plugin from {self._plugin_path}: \n&#34;
                f&#34;{e} \n&#34;
                f&#34;Using standard Yolov5 model..&#34;
            )
            return False

        try:
            # Deserialize the engine from file
            with open(self._engine_path, &#34;rb&#34;) as f:
                self.engine = self.runtime.deserialize_cuda_engine(f.read())

            # get execution context and fixed batch size
            self.context = self.engine.create_execution_context()
            self.batch_size = self.engine.max_batch_size
            return True
        except Exception as e:
            l.info(
                f&#34;Error occured while loading engine from {self._engine_path}: \n&#34;
                f&#34;{e} \n&#34;
                f&#34;Using standard Yolov5 model..&#34;
            )
            return False

    def allocate_buffers(self, is_explicit_batch=True, dynamic_shapes=[]):
        &#34;&#34;&#34;Allocates memory on the GPU according to the provided engine.

        Args:
            is_explicit_batch (bool, optional): Explicit batch flag. Defaults to True.
            dynamic_shapes (list, optional): Dynamic input shapes. Defaults to [].
        &#34;&#34;&#34;
        self.host_inputs = []
        self.cuda_inputs = []
        self.host_outputs = []
        self.cuda_outputs = []
        self.bindings = []

        for binding in self.engine:
            l.debug(
                f&#34;Binding name: {binding}, shape: {self.engine.get_binding_shape(binding)}&#34;
            )
            size = (
                trt.volume(self.engine.get_binding_shape(binding))
                * self.engine.max_batch_size
            )
            dtype = trt.nptype(self.engine.get_binding_dtype(binding))

            # Allocate host and device buffers
            host_mem = cuda.pagelocked_empty(size, dtype)
            cuda_mem = cuda.mem_alloc(host_mem.nbytes)

            # Append the device buffer to device bindings.
            self.bindings.append(int(cuda_mem))

            # Append to the appropriate list.
            if self.engine.binding_is_input(binding):
                self.input_w = self.engine.get_binding_shape(binding)[-1]
                self.input_h = self.engine.get_binding_shape(binding)[-2]

                assert (
                    self.input_w == self._required_img_size
                    and self.input_h == self._required_img_size
                ), (
                    &#34;Provided img_size in config &#34;
                    f&#34;({self._required_img_size}x{self._required_img_size}) &#34;
                    f&#34;doesn&#39;t match with the TRT engines input size &#34;
                    f&#34;({self.input_h}x{self.input_w})!&#34;
                )

                self.host_inputs.append(host_mem)
                self.cuda_inputs.append(cuda_mem)
            else:
                self.host_outputs.append(host_mem)
                self.cuda_outputs.append(cuda_mem)

    def _init_infer(self, img_size):
        &#34;&#34;&#34;Warms up the GPU.

        Args:
            img_size (Tuple): Image shape
        &#34;&#34;&#34;
        # Warm up
        iterations = 20
        sum_time = 0.0

        for _ in range(iterations):
            t1 = time.time()
            self.infer(np.zeros(img_size, dtype=np.float32))
            sum_time += time.time() - t1

        l.debug(
            f&#34;(WARM UP) avg. inference time on {img_size}: {sum_time/iterations:.5f}&#34;
        )

    @staticmethod
    def prep_image_infer(img: Union[torch.Tensor, np.array]) -&gt; np.array:
        &#34;&#34;&#34; Preprocesses images for inference \
            (expanded dim (,4), half precision (fp16), normalized)

        Args:
            img (Union[torch.Tensor, np.array]): Resized and padded image [c, w, h] or [bs, c, w, h]

        Returns:
            np.array: Normalized image
        &#34;&#34;&#34;
        if type(img) is torch.Tensor:
            img = img.cpu().numpy(np.float32)
        img = img.astype(np.float32)
        img /= 255.0
        if len(img.shape) &lt; 4:
            img = np.expand_dims(img, axis=0)
        img = np.ascontiguousarray(img)
        return img

    def prep_batches(self, imgs: np.ndarray) -&gt; List[np.ndarray]:
        &#34;&#34;&#34; Divides a contiguous array of images in [bs, c, w, h] into a List \
            of arrays [bs, c, w, h] with batch sizes compatible with the \
            TRT engine input layer.

        Args:
            imgs (np.array): Images in a single batch

        Returns:
            List[np.ndarray]: List of image batches with processable batch size
        &#34;&#34;&#34;
        modulo = imgs.shape[0] % self.batch_size

        # fill imgs array in order to be divisible by engine batch size
        if modulo != 0:
            fills = np.zeros(
                (self.batch_size - modulo, 3, imgs.shape[2], imgs.shape[3]),
                dtype=np.float32,
            )
            imgs = np.concatenate((imgs, fills))

        batched_imgs = np.vsplit(imgs, imgs.shape[0] / self.batch_size)
        return batched_imgs, modulo

    def infer(self, imgs: np.array) -&gt; List[torch.Tensor]:
        &#34;&#34;&#34;Inference wrapper

        Args:
            imgs (np.array): Resized and padded image [c, w, h] or [bs, c, w, h]

        Returns:
            List[torch.Tensor]: List of detections, on (,6) tensor [xyxy, conf, cls]
        &#34;&#34;&#34;
        # prepare imgs for inference
        # t1 = time.time()
        imgs = Yolov5TRT.prep_image_infer(imgs)
        # l.debug(f&#34;prep_image_infer: {time.time()-t1:.5f}&#34;)

        # prepare batches according to engine batch size
        # e.g. input [7, 3, 640, 640], engine bs = 4
        # prep_batches(input) -&gt; [[4, 3, 640, 640], [4, 3, 640, 640]]
        # (filled with one zero entry of [1, 3, 640, 640])
        img_batches, fills = self.prep_batches(imgs)

        # infer on batches
        # t1 = time.time()
        batched_pred = [self.infer_on_batch(batch) for batch in img_batches]
        # l.debug(f&#34;infer_on_batch: {time.time()-t1:.5f}&#34;)

        # from [num batches, batch size, 6] to [num batches x batch size, 6]
        # (prediction per image)
        pred = list(itertools.chain.from_iterable(batched_pred))
        pred = pred[:-fills] if fills &gt; 0 else pred  # only return non-fills
        return pred, None

    def infer_on_batch(self, img: np.array) -&gt; List[torch.Tensor]:
        &#34;&#34;&#34;Batched inference

        Args:
            img (np.array): Resized and padded image batch [1, c, w, h]

        Returns:
            list: List of tensors with the prediction results [xyxy, conf, cls]
        &#34;&#34;&#34;
        assert img.shape[0] == self.batch_size, (
            f&#34;Provided batch size ({img.shape[0]}) doesn&#39;t allign with &#34;
            f&#34;the engines batch size ({self.batch_size})&#34;
        )

        # get img shapes
        img_sizes = img.shape[2:]

        # restore essential components
        stream, context, engine, bindings = (
            self.stream,
            self.context,
            self.engine,
            self.bindings,
        )
        host_inputs, cuda_inputs, host_outputs, cuda_outputs = (
            self.host_inputs,
            self.cuda_inputs,
            self.host_outputs,
            self.cuda_outputs,
        )

        # copy input image to host buffer
        # np.copyto(host_inputs[0], img.ravel())
        host_inputs[0] = img.ravel()
        # start = time.time()

        # transfer input data to the GPU.
        cuda.memcpy_htod_async(cuda_inputs[0], host_inputs[0], stream)

        # run inference.
        context.execute_async(
            batch_size=self.batch_size, bindings=bindings, stream_handle=stream.handle
        )

        # transfer predictions back from the GPU.
        cuda.memcpy_dtoh_async(host_outputs[0], cuda_outputs[0], stream)

        # synchronize the stream
        stream.synchronize()

        # end = time.time()
        # l.debug(f&#34;Inf (pure) time: {end-start:.4f}&#34;)

        # here we use the first row of output in that batch_size = 1
        output = host_outputs[0]

        return [
            self.postp_image_infer(
                output[i * 6001 : (i + 1) * 6001], img_sizes[0], img_sizes[1]
            )
            for i in range(self.batch_size)
        ]

    def postp_image_infer(self, output: np.ndarray, origin_h, origin_w) -&gt; torch.Tensor:
        &#34;&#34;&#34;Reshapes the model output to interpretable shape then applies NMS

        Args:
            output (np.ndarray): A tensor like [num_boxes, cx, cy, w, h, conf, cls_id]
            origin_h ([type]): height of original image
            origin_w ([type]): width of original image

        Returns:
            torch.Tensor: tensor with the prediction results [xyxy, conf, cls]
        &#34;&#34;&#34;
        # get the num of boxes detected
        num = int(output[0])

        # reshape to a two dimentional ndarray
        pred = np.reshape(output[1:], (-1, 6))[:num, :]

        # to a torch Tensor (GPU)
        pred = torch.Tensor(pred).cuda()

        # get the boxes, scores, classid
        boxes, scores, classid = pred[:, :4], pred[:, 4], pred[:, 5]

        # choose those boxes that score &gt; CONF_THRESH
        si = scores &gt; self._yolov5._conf_thres
        boxes = boxes[si, :]
        scores = scores[si]
        classid = classid[si]

        # no detections found, return empty tensor
        if num == 0:
            return torch.zeros((0, 6))

        # transform bbox from [center_x, center_y, w, h] to [x1, y1, x2, y2]
        boxes = xywh2xyxy(boxes)

        # do nms (GPU)
        indices = torchvision.ops.nms(
            boxes, scores, iou_threshold=self._yolov5._iou_thres
        ).cpu()

        result_boxes = boxes[indices, :].cpu()
        result_scores = scores[indices].cpu()
        result_classid = classid[indices].cpu()

        result_scores = torch.unsqueeze(result_scores, 1)
        result_classid = torch.unsqueeze(result_classid, 1)

        return torch.cat((result_boxes, result_scores, result_classid), 1)</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="pancake.models.tensorrt.yolov5_trt_2.Yolov5TRT"><code class="flex name class">
<span>class <span class="ident">Yolov5TRT</span></span>
<span>(</span><span>yolov5, engine_path: str, plugin_path: str, device: str, *args, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Helper class that provides a standard way to create an ABC using
inheritance.</p>
<p>YOLOv5 TensorRT Class</p>
<h2 id="description">Description</h2>
<p>In order to use this class it is required to first have CUDA Toolkit,
CuDNN and TensorRT installed. Furthermore, you need to generate a
TensorRT engine and plugin library with this external repo:</p>
<p><a href="https://github.com/wang-xinyu/tensorrtx/tree/master/yolov5">https://github.com/wang-xinyu/tensorrtx/tree/master/yolov5</a></p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>yolov5</code></strong> :&ensp;<code>YOLOCustomDetector</code></dt>
<dd>Instance of YOLOCustomDetector</dd>
<dt><strong><code>engine_path</code></strong> :&ensp;<code>str</code></dt>
<dd>Path of the TRT engine</dd>
<dt><strong><code>plugin_path</code></strong> :&ensp;<code>str</code></dt>
<dd>Path of the TRT plugin</dd>
<dt><strong><code>device</code></strong> :&ensp;<code>str</code></dt>
<dd>Device number</dd>
</dl>
<h2 id="raises">Raises</h2>
<dl>
<dt><code>ModuleNotFoundError</code></dt>
<dd>If TensorRT library is not installed</dd>
<dt><code>ModuleNotFoundError</code></dt>
<dd>When invalid device index was provided</dd>
<dt><code>ModuleNotFoundError</code></dt>
<dd>When loading of the engine and plugin failed</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Yolov5TRT(BaseModel):
    def __init__(
        self, yolov5, engine_path: str, plugin_path: str, device: str, *args, **kwargs
    ):
        &#34;&#34;&#34; YOLOv5 TensorRT Class

        Description:
            In order to use this class it is required to first have CUDA Toolkit, \
            CuDNN and TensorRT installed. Furthermore, you need to generate a \
            TensorRT engine and plugin library with this external repo:

            https://github.com/wang-xinyu/tensorrtx/tree/master/yolov5

        Args:
            yolov5 (YOLOCustomDetector): Instance of YOLOCustomDetector
            engine_path (str): Path of the TRT engine
            plugin_path (str): Path of the TRT plugin
            device (str): Device number

        Raises:
            ModuleNotFoundError: If TensorRT library is not installed
            ModuleNotFoundError: When invalid device index was provided
            ModuleNotFoundError: When loading of the engine and plugin failed
        &#34;&#34;&#34;
        # if trt not available return standard yolov5 model
        if not trt_installed:
            l.info(&#34;TensorRT not installed, using standard Yolov5..&#34;)
            raise ModuleNotFoundError

        # store standard model
        self._yolov5 = yolov5
        self.names = self._yolov5.names
        self._required_img_size = self._yolov5._required_img_size
        self._stride = None

        # get trt engine- and plugin path
        self._engine_path = engine_path
        self._plugin_path = plugin_path

        # create a context on this device
        try:
            device = int(device)
        except:
            l.info(
                f&#34;Given device {device} was not a device index! Using standard Yolov5..&#34;
            )
            raise ModuleNotFoundError

        self.ctx = cuda.Device(device).make_context()
        self.stream = cuda.Stream()
        TRT_LOGGER = trt.Logger(trt.Logger.INFO)
        self.runtime = trt.Runtime(TRT_LOGGER)

        # loard TRT engine
        if not self.load_engine():
            raise ModuleNotFoundError

        # allocate buffers and warm up context
        self.allocate_buffers()
        self._init_infer([self.batch_size, 3, self.input_h, self.input_w])

    def load_engine(self) -&gt; bool:
        &#34;&#34;&#34;Loads the plugin library and TRT engine.

        Returns:
            bool: success flag
        &#34;&#34;&#34;
        l.info(f&#34;Loading TRT engine from {self._engine_path}..&#34;)
        try:
            # Load trt plugin lib
            import ctypes

            ctypes.CDLL(self._plugin_path)
        except Exception as e:
            l.info(
                f&#34;Error occured while loading TRT plugin from {self._plugin_path}: \n&#34;
                f&#34;{e} \n&#34;
                f&#34;Using standard Yolov5 model..&#34;
            )
            return False

        try:
            # Deserialize the engine from file
            with open(self._engine_path, &#34;rb&#34;) as f:
                self.engine = self.runtime.deserialize_cuda_engine(f.read())

            # get execution context and fixed batch size
            self.context = self.engine.create_execution_context()
            self.batch_size = self.engine.max_batch_size
            return True
        except Exception as e:
            l.info(
                f&#34;Error occured while loading engine from {self._engine_path}: \n&#34;
                f&#34;{e} \n&#34;
                f&#34;Using standard Yolov5 model..&#34;
            )
            return False

    def allocate_buffers(self, is_explicit_batch=True, dynamic_shapes=[]):
        &#34;&#34;&#34;Allocates memory on the GPU according to the provided engine.

        Args:
            is_explicit_batch (bool, optional): Explicit batch flag. Defaults to True.
            dynamic_shapes (list, optional): Dynamic input shapes. Defaults to [].
        &#34;&#34;&#34;
        self.host_inputs = []
        self.cuda_inputs = []
        self.host_outputs = []
        self.cuda_outputs = []
        self.bindings = []

        for binding in self.engine:
            l.debug(
                f&#34;Binding name: {binding}, shape: {self.engine.get_binding_shape(binding)}&#34;
            )
            size = (
                trt.volume(self.engine.get_binding_shape(binding))
                * self.engine.max_batch_size
            )
            dtype = trt.nptype(self.engine.get_binding_dtype(binding))

            # Allocate host and device buffers
            host_mem = cuda.pagelocked_empty(size, dtype)
            cuda_mem = cuda.mem_alloc(host_mem.nbytes)

            # Append the device buffer to device bindings.
            self.bindings.append(int(cuda_mem))

            # Append to the appropriate list.
            if self.engine.binding_is_input(binding):
                self.input_w = self.engine.get_binding_shape(binding)[-1]
                self.input_h = self.engine.get_binding_shape(binding)[-2]

                assert (
                    self.input_w == self._required_img_size
                    and self.input_h == self._required_img_size
                ), (
                    &#34;Provided img_size in config &#34;
                    f&#34;({self._required_img_size}x{self._required_img_size}) &#34;
                    f&#34;doesn&#39;t match with the TRT engines input size &#34;
                    f&#34;({self.input_h}x{self.input_w})!&#34;
                )

                self.host_inputs.append(host_mem)
                self.cuda_inputs.append(cuda_mem)
            else:
                self.host_outputs.append(host_mem)
                self.cuda_outputs.append(cuda_mem)

    def _init_infer(self, img_size):
        &#34;&#34;&#34;Warms up the GPU.

        Args:
            img_size (Tuple): Image shape
        &#34;&#34;&#34;
        # Warm up
        iterations = 20
        sum_time = 0.0

        for _ in range(iterations):
            t1 = time.time()
            self.infer(np.zeros(img_size, dtype=np.float32))
            sum_time += time.time() - t1

        l.debug(
            f&#34;(WARM UP) avg. inference time on {img_size}: {sum_time/iterations:.5f}&#34;
        )

    @staticmethod
    def prep_image_infer(img: Union[torch.Tensor, np.array]) -&gt; np.array:
        &#34;&#34;&#34; Preprocesses images for inference \
            (expanded dim (,4), half precision (fp16), normalized)

        Args:
            img (Union[torch.Tensor, np.array]): Resized and padded image [c, w, h] or [bs, c, w, h]

        Returns:
            np.array: Normalized image
        &#34;&#34;&#34;
        if type(img) is torch.Tensor:
            img = img.cpu().numpy(np.float32)
        img = img.astype(np.float32)
        img /= 255.0
        if len(img.shape) &lt; 4:
            img = np.expand_dims(img, axis=0)
        img = np.ascontiguousarray(img)
        return img

    def prep_batches(self, imgs: np.ndarray) -&gt; List[np.ndarray]:
        &#34;&#34;&#34; Divides a contiguous array of images in [bs, c, w, h] into a List \
            of arrays [bs, c, w, h] with batch sizes compatible with the \
            TRT engine input layer.

        Args:
            imgs (np.array): Images in a single batch

        Returns:
            List[np.ndarray]: List of image batches with processable batch size
        &#34;&#34;&#34;
        modulo = imgs.shape[0] % self.batch_size

        # fill imgs array in order to be divisible by engine batch size
        if modulo != 0:
            fills = np.zeros(
                (self.batch_size - modulo, 3, imgs.shape[2], imgs.shape[3]),
                dtype=np.float32,
            )
            imgs = np.concatenate((imgs, fills))

        batched_imgs = np.vsplit(imgs, imgs.shape[0] / self.batch_size)
        return batched_imgs, modulo

    def infer(self, imgs: np.array) -&gt; List[torch.Tensor]:
        &#34;&#34;&#34;Inference wrapper

        Args:
            imgs (np.array): Resized and padded image [c, w, h] or [bs, c, w, h]

        Returns:
            List[torch.Tensor]: List of detections, on (,6) tensor [xyxy, conf, cls]
        &#34;&#34;&#34;
        # prepare imgs for inference
        # t1 = time.time()
        imgs = Yolov5TRT.prep_image_infer(imgs)
        # l.debug(f&#34;prep_image_infer: {time.time()-t1:.5f}&#34;)

        # prepare batches according to engine batch size
        # e.g. input [7, 3, 640, 640], engine bs = 4
        # prep_batches(input) -&gt; [[4, 3, 640, 640], [4, 3, 640, 640]]
        # (filled with one zero entry of [1, 3, 640, 640])
        img_batches, fills = self.prep_batches(imgs)

        # infer on batches
        # t1 = time.time()
        batched_pred = [self.infer_on_batch(batch) for batch in img_batches]
        # l.debug(f&#34;infer_on_batch: {time.time()-t1:.5f}&#34;)

        # from [num batches, batch size, 6] to [num batches x batch size, 6]
        # (prediction per image)
        pred = list(itertools.chain.from_iterable(batched_pred))
        pred = pred[:-fills] if fills &gt; 0 else pred  # only return non-fills
        return pred, None

    def infer_on_batch(self, img: np.array) -&gt; List[torch.Tensor]:
        &#34;&#34;&#34;Batched inference

        Args:
            img (np.array): Resized and padded image batch [1, c, w, h]

        Returns:
            list: List of tensors with the prediction results [xyxy, conf, cls]
        &#34;&#34;&#34;
        assert img.shape[0] == self.batch_size, (
            f&#34;Provided batch size ({img.shape[0]}) doesn&#39;t allign with &#34;
            f&#34;the engines batch size ({self.batch_size})&#34;
        )

        # get img shapes
        img_sizes = img.shape[2:]

        # restore essential components
        stream, context, engine, bindings = (
            self.stream,
            self.context,
            self.engine,
            self.bindings,
        )
        host_inputs, cuda_inputs, host_outputs, cuda_outputs = (
            self.host_inputs,
            self.cuda_inputs,
            self.host_outputs,
            self.cuda_outputs,
        )

        # copy input image to host buffer
        # np.copyto(host_inputs[0], img.ravel())
        host_inputs[0] = img.ravel()
        # start = time.time()

        # transfer input data to the GPU.
        cuda.memcpy_htod_async(cuda_inputs[0], host_inputs[0], stream)

        # run inference.
        context.execute_async(
            batch_size=self.batch_size, bindings=bindings, stream_handle=stream.handle
        )

        # transfer predictions back from the GPU.
        cuda.memcpy_dtoh_async(host_outputs[0], cuda_outputs[0], stream)

        # synchronize the stream
        stream.synchronize()

        # end = time.time()
        # l.debug(f&#34;Inf (pure) time: {end-start:.4f}&#34;)

        # here we use the first row of output in that batch_size = 1
        output = host_outputs[0]

        return [
            self.postp_image_infer(
                output[i * 6001 : (i + 1) * 6001], img_sizes[0], img_sizes[1]
            )
            for i in range(self.batch_size)
        ]

    def postp_image_infer(self, output: np.ndarray, origin_h, origin_w) -&gt; torch.Tensor:
        &#34;&#34;&#34;Reshapes the model output to interpretable shape then applies NMS

        Args:
            output (np.ndarray): A tensor like [num_boxes, cx, cy, w, h, conf, cls_id]
            origin_h ([type]): height of original image
            origin_w ([type]): width of original image

        Returns:
            torch.Tensor: tensor with the prediction results [xyxy, conf, cls]
        &#34;&#34;&#34;
        # get the num of boxes detected
        num = int(output[0])

        # reshape to a two dimentional ndarray
        pred = np.reshape(output[1:], (-1, 6))[:num, :]

        # to a torch Tensor (GPU)
        pred = torch.Tensor(pred).cuda()

        # get the boxes, scores, classid
        boxes, scores, classid = pred[:, :4], pred[:, 4], pred[:, 5]

        # choose those boxes that score &gt; CONF_THRESH
        si = scores &gt; self._yolov5._conf_thres
        boxes = boxes[si, :]
        scores = scores[si]
        classid = classid[si]

        # no detections found, return empty tensor
        if num == 0:
            return torch.zeros((0, 6))

        # transform bbox from [center_x, center_y, w, h] to [x1, y1, x2, y2]
        boxes = xywh2xyxy(boxes)

        # do nms (GPU)
        indices = torchvision.ops.nms(
            boxes, scores, iou_threshold=self._yolov5._iou_thres
        ).cpu()

        result_boxes = boxes[indices, :].cpu()
        result_scores = scores[indices].cpu()
        result_classid = classid[indices].cpu()

        result_scores = torch.unsqueeze(result_scores, 1)
        result_classid = torch.unsqueeze(result_classid, 1)

        return torch.cat((result_boxes, result_scores, result_classid), 1)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="pancake.models.base_class.BaseModel" href="../base_class.html#pancake.models.base_class.BaseModel">BaseModel</a></li>
<li>abc.ABC</li>
</ul>
<h3>Static methods</h3>
<dl>
<dt id="pancake.models.tensorrt.yolov5_trt_2.Yolov5TRT.prep_image_infer"><code class="name flex">
<span>def <span class="ident">prep_image_infer</span></span>(<span>img: Union[torch.Tensor, <built-in function array>]) ‑> <built-in function array></span>
</code></dt>
<dd>
<div class="desc"><p>Preprocesses images for inference
(expanded dim (,4), half precision (fp16), normalized)</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>img</code></strong> :&ensp;<code>Union[torch.Tensor, np.array]</code></dt>
<dd>Resized and padded image [c, w, h] or [bs, c, w, h]</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>np.array</code></dt>
<dd>Normalized image</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def prep_image_infer(img: Union[torch.Tensor, np.array]) -&gt; np.array:
    &#34;&#34;&#34; Preprocesses images for inference \
        (expanded dim (,4), half precision (fp16), normalized)

    Args:
        img (Union[torch.Tensor, np.array]): Resized and padded image [c, w, h] or [bs, c, w, h]

    Returns:
        np.array: Normalized image
    &#34;&#34;&#34;
    if type(img) is torch.Tensor:
        img = img.cpu().numpy(np.float32)
    img = img.astype(np.float32)
    img /= 255.0
    if len(img.shape) &lt; 4:
        img = np.expand_dims(img, axis=0)
    img = np.ascontiguousarray(img)
    return img</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="pancake.models.tensorrt.yolov5_trt_2.Yolov5TRT.allocate_buffers"><code class="name flex">
<span>def <span class="ident">allocate_buffers</span></span>(<span>self, is_explicit_batch=True, dynamic_shapes=[])</span>
</code></dt>
<dd>
<div class="desc"><p>Allocates memory on the GPU according to the provided engine.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>is_explicit_batch</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>Explicit batch flag. Defaults to True.</dd>
<dt><strong><code>dynamic_shapes</code></strong> :&ensp;<code>list</code>, optional</dt>
<dd>Dynamic input shapes. Defaults to [].</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def allocate_buffers(self, is_explicit_batch=True, dynamic_shapes=[]):
    &#34;&#34;&#34;Allocates memory on the GPU according to the provided engine.

    Args:
        is_explicit_batch (bool, optional): Explicit batch flag. Defaults to True.
        dynamic_shapes (list, optional): Dynamic input shapes. Defaults to [].
    &#34;&#34;&#34;
    self.host_inputs = []
    self.cuda_inputs = []
    self.host_outputs = []
    self.cuda_outputs = []
    self.bindings = []

    for binding in self.engine:
        l.debug(
            f&#34;Binding name: {binding}, shape: {self.engine.get_binding_shape(binding)}&#34;
        )
        size = (
            trt.volume(self.engine.get_binding_shape(binding))
            * self.engine.max_batch_size
        )
        dtype = trt.nptype(self.engine.get_binding_dtype(binding))

        # Allocate host and device buffers
        host_mem = cuda.pagelocked_empty(size, dtype)
        cuda_mem = cuda.mem_alloc(host_mem.nbytes)

        # Append the device buffer to device bindings.
        self.bindings.append(int(cuda_mem))

        # Append to the appropriate list.
        if self.engine.binding_is_input(binding):
            self.input_w = self.engine.get_binding_shape(binding)[-1]
            self.input_h = self.engine.get_binding_shape(binding)[-2]

            assert (
                self.input_w == self._required_img_size
                and self.input_h == self._required_img_size
            ), (
                &#34;Provided img_size in config &#34;
                f&#34;({self._required_img_size}x{self._required_img_size}) &#34;
                f&#34;doesn&#39;t match with the TRT engines input size &#34;
                f&#34;({self.input_h}x{self.input_w})!&#34;
            )

            self.host_inputs.append(host_mem)
            self.cuda_inputs.append(cuda_mem)
        else:
            self.host_outputs.append(host_mem)
            self.cuda_outputs.append(cuda_mem)</code></pre>
</details>
</dd>
<dt id="pancake.models.tensorrt.yolov5_trt_2.Yolov5TRT.infer"><code class="name flex">
<span>def <span class="ident">infer</span></span>(<span>self, imgs: <built-in function array>) ‑> List[torch.Tensor]</span>
</code></dt>
<dd>
<div class="desc"><p>Inference wrapper</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>imgs</code></strong> :&ensp;<code>np.array</code></dt>
<dd>Resized and padded image [c, w, h] or [bs, c, w, h]</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>List[torch.Tensor]</code></dt>
<dd>List of detections, on (,6) tensor [xyxy, conf, cls]</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def infer(self, imgs: np.array) -&gt; List[torch.Tensor]:
    &#34;&#34;&#34;Inference wrapper

    Args:
        imgs (np.array): Resized and padded image [c, w, h] or [bs, c, w, h]

    Returns:
        List[torch.Tensor]: List of detections, on (,6) tensor [xyxy, conf, cls]
    &#34;&#34;&#34;
    # prepare imgs for inference
    # t1 = time.time()
    imgs = Yolov5TRT.prep_image_infer(imgs)
    # l.debug(f&#34;prep_image_infer: {time.time()-t1:.5f}&#34;)

    # prepare batches according to engine batch size
    # e.g. input [7, 3, 640, 640], engine bs = 4
    # prep_batches(input) -&gt; [[4, 3, 640, 640], [4, 3, 640, 640]]
    # (filled with one zero entry of [1, 3, 640, 640])
    img_batches, fills = self.prep_batches(imgs)

    # infer on batches
    # t1 = time.time()
    batched_pred = [self.infer_on_batch(batch) for batch in img_batches]
    # l.debug(f&#34;infer_on_batch: {time.time()-t1:.5f}&#34;)

    # from [num batches, batch size, 6] to [num batches x batch size, 6]
    # (prediction per image)
    pred = list(itertools.chain.from_iterable(batched_pred))
    pred = pred[:-fills] if fills &gt; 0 else pred  # only return non-fills
    return pred, None</code></pre>
</details>
</dd>
<dt id="pancake.models.tensorrt.yolov5_trt_2.Yolov5TRT.infer_on_batch"><code class="name flex">
<span>def <span class="ident">infer_on_batch</span></span>(<span>self, img: <built-in function array>) ‑> List[torch.Tensor]</span>
</code></dt>
<dd>
<div class="desc"><p>Batched inference</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>img</code></strong> :&ensp;<code>np.array</code></dt>
<dd>Resized and padded image batch [1, c, w, h]</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>list</code></dt>
<dd>List of tensors with the prediction results [xyxy, conf, cls]</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def infer_on_batch(self, img: np.array) -&gt; List[torch.Tensor]:
    &#34;&#34;&#34;Batched inference

    Args:
        img (np.array): Resized and padded image batch [1, c, w, h]

    Returns:
        list: List of tensors with the prediction results [xyxy, conf, cls]
    &#34;&#34;&#34;
    assert img.shape[0] == self.batch_size, (
        f&#34;Provided batch size ({img.shape[0]}) doesn&#39;t allign with &#34;
        f&#34;the engines batch size ({self.batch_size})&#34;
    )

    # get img shapes
    img_sizes = img.shape[2:]

    # restore essential components
    stream, context, engine, bindings = (
        self.stream,
        self.context,
        self.engine,
        self.bindings,
    )
    host_inputs, cuda_inputs, host_outputs, cuda_outputs = (
        self.host_inputs,
        self.cuda_inputs,
        self.host_outputs,
        self.cuda_outputs,
    )

    # copy input image to host buffer
    # np.copyto(host_inputs[0], img.ravel())
    host_inputs[0] = img.ravel()
    # start = time.time()

    # transfer input data to the GPU.
    cuda.memcpy_htod_async(cuda_inputs[0], host_inputs[0], stream)

    # run inference.
    context.execute_async(
        batch_size=self.batch_size, bindings=bindings, stream_handle=stream.handle
    )

    # transfer predictions back from the GPU.
    cuda.memcpy_dtoh_async(host_outputs[0], cuda_outputs[0], stream)

    # synchronize the stream
    stream.synchronize()

    # end = time.time()
    # l.debug(f&#34;Inf (pure) time: {end-start:.4f}&#34;)

    # here we use the first row of output in that batch_size = 1
    output = host_outputs[0]

    return [
        self.postp_image_infer(
            output[i * 6001 : (i + 1) * 6001], img_sizes[0], img_sizes[1]
        )
        for i in range(self.batch_size)
    ]</code></pre>
</details>
</dd>
<dt id="pancake.models.tensorrt.yolov5_trt_2.Yolov5TRT.load_engine"><code class="name flex">
<span>def <span class="ident">load_engine</span></span>(<span>self) ‑> bool</span>
</code></dt>
<dd>
<div class="desc"><p>Loads the plugin library and TRT engine.</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>bool</code></dt>
<dd>success flag</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def load_engine(self) -&gt; bool:
    &#34;&#34;&#34;Loads the plugin library and TRT engine.

    Returns:
        bool: success flag
    &#34;&#34;&#34;
    l.info(f&#34;Loading TRT engine from {self._engine_path}..&#34;)
    try:
        # Load trt plugin lib
        import ctypes

        ctypes.CDLL(self._plugin_path)
    except Exception as e:
        l.info(
            f&#34;Error occured while loading TRT plugin from {self._plugin_path}: \n&#34;
            f&#34;{e} \n&#34;
            f&#34;Using standard Yolov5 model..&#34;
        )
        return False

    try:
        # Deserialize the engine from file
        with open(self._engine_path, &#34;rb&#34;) as f:
            self.engine = self.runtime.deserialize_cuda_engine(f.read())

        # get execution context and fixed batch size
        self.context = self.engine.create_execution_context()
        self.batch_size = self.engine.max_batch_size
        return True
    except Exception as e:
        l.info(
            f&#34;Error occured while loading engine from {self._engine_path}: \n&#34;
            f&#34;{e} \n&#34;
            f&#34;Using standard Yolov5 model..&#34;
        )
        return False</code></pre>
</details>
</dd>
<dt id="pancake.models.tensorrt.yolov5_trt_2.Yolov5TRT.postp_image_infer"><code class="name flex">
<span>def <span class="ident">postp_image_infer</span></span>(<span>self, output: numpy.ndarray, origin_h, origin_w) ‑> torch.Tensor</span>
</code></dt>
<dd>
<div class="desc"><p>Reshapes the model output to interpretable shape then applies NMS</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>output</code></strong> :&ensp;<code>np.ndarray</code></dt>
<dd>A tensor like [num_boxes, cx, cy, w, h, conf, cls_id]</dd>
<dt><strong><code>origin_h</code></strong> :&ensp;<code>[type]</code></dt>
<dd>height of original image</dd>
<dt><strong><code>origin_w</code></strong> :&ensp;<code>[type]</code></dt>
<dd>width of original image</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>torch.Tensor</code></dt>
<dd>tensor with the prediction results [xyxy, conf, cls]</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def postp_image_infer(self, output: np.ndarray, origin_h, origin_w) -&gt; torch.Tensor:
    &#34;&#34;&#34;Reshapes the model output to interpretable shape then applies NMS

    Args:
        output (np.ndarray): A tensor like [num_boxes, cx, cy, w, h, conf, cls_id]
        origin_h ([type]): height of original image
        origin_w ([type]): width of original image

    Returns:
        torch.Tensor: tensor with the prediction results [xyxy, conf, cls]
    &#34;&#34;&#34;
    # get the num of boxes detected
    num = int(output[0])

    # reshape to a two dimentional ndarray
    pred = np.reshape(output[1:], (-1, 6))[:num, :]

    # to a torch Tensor (GPU)
    pred = torch.Tensor(pred).cuda()

    # get the boxes, scores, classid
    boxes, scores, classid = pred[:, :4], pred[:, 4], pred[:, 5]

    # choose those boxes that score &gt; CONF_THRESH
    si = scores &gt; self._yolov5._conf_thres
    boxes = boxes[si, :]
    scores = scores[si]
    classid = classid[si]

    # no detections found, return empty tensor
    if num == 0:
        return torch.zeros((0, 6))

    # transform bbox from [center_x, center_y, w, h] to [x1, y1, x2, y2]
    boxes = xywh2xyxy(boxes)

    # do nms (GPU)
    indices = torchvision.ops.nms(
        boxes, scores, iou_threshold=self._yolov5._iou_thres
    ).cpu()

    result_boxes = boxes[indices, :].cpu()
    result_scores = scores[indices].cpu()
    result_classid = classid[indices].cpu()

    result_scores = torch.unsqueeze(result_scores, 1)
    result_classid = torch.unsqueeze(result_classid, 1)

    return torch.cat((result_boxes, result_scores, result_classid), 1)</code></pre>
</details>
</dd>
<dt id="pancake.models.tensorrt.yolov5_trt_2.Yolov5TRT.prep_batches"><code class="name flex">
<span>def <span class="ident">prep_batches</span></span>(<span>self, imgs: numpy.ndarray) ‑> List[numpy.ndarray]</span>
</code></dt>
<dd>
<div class="desc"><p>Divides a contiguous array of images in [bs, c, w, h] into a List
of arrays [bs, c, w, h] with batch sizes compatible with the
TRT engine input layer.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>imgs</code></strong> :&ensp;<code>np.array</code></dt>
<dd>Images in a single batch</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>List[np.ndarray]</code></dt>
<dd>List of image batches with processable batch size</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def prep_batches(self, imgs: np.ndarray) -&gt; List[np.ndarray]:
    &#34;&#34;&#34; Divides a contiguous array of images in [bs, c, w, h] into a List \
        of arrays [bs, c, w, h] with batch sizes compatible with the \
        TRT engine input layer.

    Args:
        imgs (np.array): Images in a single batch

    Returns:
        List[np.ndarray]: List of image batches with processable batch size
    &#34;&#34;&#34;
    modulo = imgs.shape[0] % self.batch_size

    # fill imgs array in order to be divisible by engine batch size
    if modulo != 0:
        fills = np.zeros(
            (self.batch_size - modulo, 3, imgs.shape[2], imgs.shape[3]),
            dtype=np.float32,
        )
        imgs = np.concatenate((imgs, fills))

    batched_imgs = np.vsplit(imgs, imgs.shape[0] / self.batch_size)
    return batched_imgs, modulo</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="pancake.models.tensorrt" href="index.html">pancake.models.tensorrt</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="pancake.models.tensorrt.yolov5_trt_2.Yolov5TRT" href="#pancake.models.tensorrt.yolov5_trt_2.Yolov5TRT">Yolov5TRT</a></code></h4>
<ul class="two-column">
<li><code><a title="pancake.models.tensorrt.yolov5_trt_2.Yolov5TRT.allocate_buffers" href="#pancake.models.tensorrt.yolov5_trt_2.Yolov5TRT.allocate_buffers">allocate_buffers</a></code></li>
<li><code><a title="pancake.models.tensorrt.yolov5_trt_2.Yolov5TRT.infer" href="#pancake.models.tensorrt.yolov5_trt_2.Yolov5TRT.infer">infer</a></code></li>
<li><code><a title="pancake.models.tensorrt.yolov5_trt_2.Yolov5TRT.infer_on_batch" href="#pancake.models.tensorrt.yolov5_trt_2.Yolov5TRT.infer_on_batch">infer_on_batch</a></code></li>
<li><code><a title="pancake.models.tensorrt.yolov5_trt_2.Yolov5TRT.load_engine" href="#pancake.models.tensorrt.yolov5_trt_2.Yolov5TRT.load_engine">load_engine</a></code></li>
<li><code><a title="pancake.models.tensorrt.yolov5_trt_2.Yolov5TRT.postp_image_infer" href="#pancake.models.tensorrt.yolov5_trt_2.Yolov5TRT.postp_image_infer">postp_image_infer</a></code></li>
<li><code><a title="pancake.models.tensorrt.yolov5_trt_2.Yolov5TRT.prep_batches" href="#pancake.models.tensorrt.yolov5_trt_2.Yolov5TRT.prep_batches">prep_batches</a></code></li>
<li><code><a title="pancake.models.tensorrt.yolov5_trt_2.Yolov5TRT.prep_image_infer" href="#pancake.models.tensorrt.yolov5_trt_2.Yolov5TRT.prep_image_infer">prep_image_infer</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.9.2</a>.</p>
</footer>
</body>
</html>