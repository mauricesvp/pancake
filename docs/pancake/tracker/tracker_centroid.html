<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.9.2" />
<title>pancake.tracker.tracker_centroid API documentation</title>
<meta name="description" content="Tracker based on Centroid tracking." />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>pancake.tracker.tracker_centroid</code></h1>
</header>
<section id="section-intro">
<p>Tracker based on Centroid tracking.</p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">&#34;&#34;&#34;Tracker based on Centroid tracking.&#34;&#34;&#34;
from .tracker import BaseTracker
from typing import Type, Tuple

from pancake.logger import setup_logger

from collections import OrderedDict
import numpy as np
import torch
import time
import math

from shapely.geometry import Point
from shapely.geometry.polygon import Polygon

from scipy.spatial import distance as dist
from operator import itemgetter


class CentroidTracker(BaseTracker):
    def __init__(self, cfg: dict, *args, **kwargs) -&gt; None:
        &#34;&#34;&#34;Initializes the CentroidTracker

        Description:
            Sets up the internal variables and allocates the OrderedDicts for further processing.

        Args:
            cfg (dict): dictionary including all configuration variables.
        &#34;&#34;&#34;        
        self.l = setup_logger(__name__)  # You can use any name
        self.l.debug(&#34;INIT CENTROID TRACKER - Final&#34;)
        # options
        self.USE_BETTER_RECTS = cfg.CENTROID.USE_BETTER_RECTS
        self.USE_DYNAMIC_SCALING = cfg.CENTROID.USE_DYNAMIC_SCALING
        # constants
        self.MAX_ID = cfg.CENTROID.MAX_ID
        self.MAX_DISAPPEARED = cfg.CENTROID.MAX_DISAPPEARED
        # car match tolerances
        self.DISTANCE_TOLERANCE = cfg.CENTROID.DISTANCE_TOLERANCE
        self.VERTICAL_TOLERANCE = cfg.CENTROID.VERTICAL_TOLERANCE
        self.FRONT_DISTANCE_TOLERANCE = cfg.CENTROID.FRONT_DISTANCE_TOLERANCE
        self.BACK_DISTANCE_TOLERANCE = cfg.CENTROID.BACK_DISTANCE_TOLERANCE
        self.SIDE_DISTANCE_TOLERANCE = cfg.CENTROID.SIDE_DISTANCE_TOLERANCE
        # image boundaries regions
        self.FRAME_WIDTH = cfg.CENTROID.FRAME_WIDTH
        self.FRAME_CHANGE_LC = self.FRAME_WIDTH // 3
        self.FRAME_CHANGE_CR = self.FRAME_CHANGE_LC * 2
        # image transition region size
        self.TRANSITION_WIDTH = cfg.CENTROID.TRANSITION_WIDTH
        # lane separators
        self.LANE_SEPARATOR_LL = cfg.CENTROID.LANE_SEPARATOR_LL
        self.LANE_SEPARATOR_LC = cfg.CENTROID.LANE_SEPARATOR_LC
        self.LANE_SEPARATOR_CR = cfg.CENTROID.LANE_SEPARATOR_CR
        self.LANE_SEPARATOR_RR = cfg.CENTROID.LANE_SEPARATOR_RR
        # deregistration zone boundaries
        self.DEREG_ZONE_L = cfg.CENTROID.DEREG_ZONE_L
        self.DEREG_ZONE_R = cfg.CENTROID.DEREG_ZONE_R
        # registration zone boundaries
        self.REG_ZONE_L = cfg.CENTROID.REG_ZONE_L
        self.REG_ZONE_R = cfg.CENTROID.REG_ZONE_R
        # initialize the next unique object ID along with two ordered
        # dictionaries used to keep track of mapping a given object
        # ID to its centroid and number of consecutive frames it has
        # been marked as &#34;disappeared&#34;, respectively
        self.nextObjectID = 0
        self.objects = OrderedDict()
        self.disappeared = OrderedDict()
        # dicts for return types
        self.bbox = OrderedDict()
        self.confidence = OrderedDict()
        self.classIds = OrderedDict()
        # known previous distant travelled
        self.lastDistTrav = OrderedDict()

    def _centroid(self, vertices: np.ndarray) -&gt; Tuple[int, int]:
        &#34;&#34;&#34;Calculates a centroid from the BBOX coordinates

        Description:
            May receive an unlimited amount of coordinates. But these coordinates need to alternate between x and y.

        Args:
            vertices (np.ndarray): array of x and y coordinates

        Returns:
            Tuple[int, int]: tuple describing the x and y coordinates on the image
        &#34;&#34;&#34;        
        x_list = [vertex for vertex in vertices[::2]]
        y_list = [vertex for vertex in vertices[1::2]]
        x = int(sum(x_list) // len(x_list))
        y = int(sum(y_list) // len(y_list))
        return x, y

    def _register(self, centroid: np.ndarray, bbox: np.ndarray, conf: np.ndarray, cls: np.ndarray):
        &#34;&#34;&#34;Registers an object

        Args:
            centroid (np.ndarray): 2 element array of the coordinates of the object
            bbox (np.ndarray): 4 element array of the top left and bottom right bounding box limits of the object
            conf (np.ndarray): 1 element array of the confidence of the object
            cls (np.ndarray): 1 element array of the class of the object
        &#34;&#34;&#34;        
        # when registering an object we use the next available object
        # ID to store the centroid
        self.objects[self.nextObjectID] = centroid
        self.disappeared[self.nextObjectID] = 0
        # dont add lastDistTrav
        # add other information
        self.bbox[self.nextObjectID] = bbox
        self.confidence[self.nextObjectID] = conf
        self.classIds[self.nextObjectID] = cls
        # increment objectID
        self.nextObjectID += 1
        if self.nextObjectID &gt;= self.MAX_ID:
            self.nextObjectID = 0

    def _deregister(self, objectID: int):
        &#34;&#34;&#34;Deletes an object by ID

        Args:
            objectID (int): ID of the object to be deleted
        &#34;&#34;&#34;        
        # to deregister an object ID we delete the object ID from
        # all of our respective dictionaries
        del self.objects[objectID]
        del self.disappeared[objectID]
        del self.bbox[objectID]
        del self.confidence[objectID]
        del self.classIds[objectID]
        try:
            del self.lastDistTrav[objectID]
        except Exception:
            pass

    def _return(self) -&gt; np.ndarray:
        &#34;&#34;&#34;Returns the Centroids with their respective data as required by the update function.

        Returns:
            np.ndarray: Tracked entities in [x1, y1, x2, y2, centre x, centre y, id, cls id]
        &#34;&#34;&#34;        
        outputs = []
        for id in list(self.objects.keys()):
            outputs.append(
                np.array(
                    [
                        self.bbox[id][0],
                        self.bbox[id][1],
                        self.bbox[id][2],
                        self.bbox[id][3],
                        self.objects[id][0],
                        self.objects[id][1],
                        id,
                        self.classIds[id],
                    ],
                    dtype=np.int,
                )
            )
        # output np array
        if len(outputs) &gt; 0:
            outputs = np.stack(outputs, axis=0)
        return outputs

    def _computeLastDist(self, prevPos: np.ndarray, currPos: np.ndarray) -&gt; Tuple[int,int]:
        &#34;&#34;&#34;Computes the distance vector between two points

        Description:
            Used as a measure between the last known position and the predicted next position

        Args:
            prevPos (np.ndarray): 2 element array of the x and y coordinates in the image of the previously known position (first position)
            currPos (np.ndarray): 2 element array of the x and y coordinates in the image of the currently known position (second position)

        Returns:
            Tuple[int,int]: 2 element tuple of the x and y coordinates in the image of the vector from the previous to the current position
        &#34;&#34;&#34;        
        return currPos[0] - prevPos[0], currPos[1] - prevPos[1]

    def _isAboveLaneSeparator(self, centroid: np.ndarray) -&gt; bool:
        &#34;&#34;&#34;Checks if a centroid is on the upper or lower lane on the image

        Args:
            centroid (np.ndarray): 2 element array of the x and y coordinates on the image of the checked position

        Returns:
            bool: Returns if the input centroid is on the upper (True) or lower (False) driving lane
        &#34;&#34;&#34;        
        if centroid[0] &lt; self.FRAME_CHANGE_LC:
            # left image
            lanesepvec = (
                self.FRAME_CHANGE_LC - 0,
                self.LANE_SEPARATOR_LC - self.LANE_SEPARATOR_LL,
            )
            centrvec = (
                self.FRAME_CHANGE_LC - centroid[0],
                self.LANE_SEPARATOR_LC - centroid[1],
            )
        elif centroid[0] &gt; self.FRAME_CHANGE_CR:
            # right image
            lanesepvec = (
                self.FRAME_WIDTH - self.FRAME_CHANGE_CR,
                self.LANE_SEPARATOR_RR - self.LANE_SEPARATOR_CR,
            )
            centrvec = (
                self.FRAME_WIDTH - centroid[0],
                self.LANE_SEPARATOR_RR - centroid[1],
            )
        else:
            # center image
            lanesepvec = (
                self.FRAME_CHANGE_CR - self.FRAME_CHANGE_LC,
                self.LANE_SEPARATOR_CR - self.LANE_SEPARATOR_LC,
            )
            centrvec = (
                self.FRAME_CHANGE_CR - centroid[0],
                self.LANE_SEPARATOR_CR - centroid[1],
            )
        # calculate cross product in 2d
        cross_product = lanesepvec[0] * centrvec[1] - lanesepvec[1] * centrvec[0]
        return cross_product &gt; 0

    def _isInsideDeregistrationZone(self, centroid: np.ndarray) -&gt; bool:
        &#34;&#34;&#34;Determines whether a centroid is inside the deregistration zone on the left and right cameras edges

        Args:
            centroid (np.ndarray): 2 element array of the x and y coordinates on the image of the position being checked

        Returns:
            bool: Returns whether the input centroid is inside the deregistration zone
        &#34;&#34;&#34;
        if centroid[0] &lt; self.DEREG_ZONE_L:
            # left dereg
            return self._isAboveLaneSeparator(centroid)
        elif centroid[0] &gt; self.DEREG_ZONE_R:
            # right dereg
            return not self._isAboveLaneSeparator(centroid)
        else:
            # between deregs
            return False

    def _isInsideRegistrationZone(self, centroid: np. ndarray) -&gt; bool:
        &#34;&#34;&#34;Determines whether a centroid is inside the registration zone on the left and right cameras edges

        Args:
            centroid (np.ndarray): 2 element array of the x and y coordinates on the image of the position being checked

        Returns:
            bool: Returns whether the input centroid is inside the registration zone
        &#34;&#34;&#34;
        if centroid[0] &lt; self.REG_ZONE_L:
            # left reg
            return not self._isAboveLaneSeparator(centroid)
        elif centroid[0] &gt; self.REG_ZONE_R:
            # right reg
            return self._isAboveLaneSeparator(centroid)
        else:
            # between regs
            return False

    def _isInsideTransitionZone(self, centroid: np.ndarray) -&gt; bool:
        &#34;&#34;&#34;Determines whether a centroid is inside the transition region between camera images

        Args:
            centroid (np.ndarray): 2 element array of the x and y coordinates on the image of the position being checked

        Returns:
            bool: Returns whether the input centroid is inside the transition region
        &#34;&#34;&#34;        
        if (
            centroid[0] &lt; self.FRAME_CHANGE_LC + self.TRANSITION_WIDTH
            and centroid[0] &gt; self.FRAME_CHANGE_LC - self.TRANSITION_WIDTH
        ):
            # left transition region
            return True
        elif (
            centroid[0] &lt; self.FRAME_CHANGE_CR + self.TRANSITION_WIDTH
            and centroid[0] &gt; self.FRAME_CHANGE_CR - self.TRANSITION_WIDTH
        ):
            # right transition region
            return True
        else:
            # not in transition regions
            return False

    def _getLen(self, tup: Tuple[int, int]) -&gt; float:
        &#34;&#34;&#34;Gets the length of a vector depicted as a tuple

        Args:
            tup (Tuple[int, int]): 2 element tuple describing a vector

        Returns:
            float: length of the vector
        &#34;&#34;&#34;        
        return (tup[0] ** 2 + tup[1] ** 2) ** 0.5

    def _continueMovement(self, objectID: int) -&gt; Tuple[int, int]:
        &#34;&#34;&#34;Continues the movement of a specific object.

        Description:
            Moves an object - depicted by its objectID - to a position where it is predicted to appear in the future.
            This is done by using the last know movement.

        Args:
            objectID (int): ID of the object that should be moved.

        Returns:
            Tuple[int, int]: 2 element tuple of the new predicted position
        &#34;&#34;&#34;        
        objPos = self.objects[objectID]

        try:
            distance = self.lastDistTrav[objectID]
        except Exception:  # object was only seen one frame
            distance = (0, 0)

        if self._isInsideTransitionZone(objPos):
            if objPos[0] &lt; self.FRAME_CHANGE_LC:
                # follow left vector
                dirVect = (
                    0 - self.FRAME_CHANGE_LC,
                    self.LANE_SEPARATOR_LL - self.LANE_SEPARATOR_LC,
                )
            elif objPos[0] &gt; self.FRAME_CHANGE_CR:
                # follow right vector
                dirVect = (
                    self.FRAME_CHANGE_CR - self.FRAME_WIDTH,
                    self.LANE_SEPARATOR_CR - self.LANE_SEPARATOR_RR,
                )
            else:
                # follow center vector
                dirVect = (
                    self.FRAME_CHANGE_LC - self.FRAME_CHANGE_CR,
                    self.LANE_SEPARATOR_LC - self.LANE_SEPARATOR_CR,
                )

            # flip direction of dirVect for bottom lane
            if not self._isAboveLaneSeparator(objPos):
                dirVect = (-dirVect[0], -dirVect[1])

            # change distance according to dirVect
            lenDistance = self._getLen(distance)
            dirVectLen = self._getLen(dirVect)
            dirVectNorm = (dirVect[0] / dirVectLen, dirVect[1] / dirVectLen)
            distance = (dirVectNorm[0] * lenDistance, dirVectNorm[1] * lenDistance)

        predmove = objPos[0] + distance[0], objPos[1] + distance[1]
        return predmove

    def _isInsideRect(self, currpt: np.ndarray, predpt: Tuple[int, int], matchpt: np.ndarray, objID: int) -&gt; bool:
        &#34;&#34;&#34;Checks if matchpt is inside a rectangle

        Description:
            The rectangle is generated by the current position (currpt) and the predicted position (predpt) with some dynamically or statically adjustable padding

        Args:
            currpt (np.ndarray): 2 element array of the x and y coordinates on the image of the current position
            predpt (Tuple[int, int]): 2 element tuple of the x and y coordinates on the image of the predicted position
            matchpt (np.ndarray): 2 element array of the x and y coordinates on the image of the potential match
            objID (int): ID of the object currently inspected

        Returns:
            bool: Returns if matchpt is inside a rectangle
        &#34;&#34;&#34;        
        if self.USE_DYNAMIC_SCALING:
            # get scaling based on camera
            if currpt[0] &lt; self.FRAME_CHANGE_LC:
                # left camera
                objectSize = 0.0863384 * currpt[0] - 109.93
            elif currpt[0] &gt; self.FRAME_CHANGE_CR:
                # right camera
                objectSize = -0.060652 * currpt[0] + 676.94
            else:
                # center camera
                objectSize = 228
            # normalize scale to center camera
            scale = objectSize / 228
        else:
            scale = 1

        # set scaling
        vectorScale = scale
        paddingScale = scale

        # get vector from currpt to predpt
        predVect = (predpt[0] - currpt[0], predpt[1] - currpt[1])

        # set own prediction vector if no prediction was made previously
        if predVect == (0,0):
            if currpt[0] &lt; self.FRAME_CHANGE_LC:
                # follow left vector
                predVectTmp = (
                    0 - self.FRAME_CHANGE_LC,
                    self.LANE_SEPARATOR_LL - self.LANE_SEPARATOR_LC,
                )
            elif currpt[0] &gt; self.FRAME_CHANGE_CR:
                # follow right vector
                predVectTmp = (
                    self.FRAME_CHANGE_CR - self.FRAME_WIDTH,
                    self.LANE_SEPARATOR_CR - self.LANE_SEPARATOR_RR,
                )
            else:
                # follow center vector
                predVectTmp = (
                    self.FRAME_CHANGE_LC - self.FRAME_CHANGE_CR,
                    self.LANE_SEPARATOR_LC - self.LANE_SEPARATOR_CR,
                )

            # flip direction of predVectTmp for bottom lane
            if not self._isAboveLaneSeparator(currpt):
                predVectTmp = (-predVectTmp[0], -predVectTmp[1])

            # normalize predVectTmp
            predVectLen = self._getLen(predVectTmp)
            predVectNorm = (predVectTmp[0] / predVectLen, predVectTmp[1] / predVectLen)

            # generate predVect
            predVect = (predVectNorm[0] * vectorScale, predVectNorm[1] * vectorScale)

        # calculate prediction vectors to generate the rectangle from
        predVectLen    = self._getLen(predVect)
        predVectNorm   = (predVect[0] / predVectLen, predVect[1] / predVectLen)
        predVect90Norm = (-predVectNorm[1], predVectNorm[0])

        # calculate padding vectors
        frontPadding  = (self.FRONT_DISTANCE_TOLERANCE * paddingScale *  predVectNorm[0],
                         self.FRONT_DISTANCE_TOLERANCE * paddingScale *  predVectNorm[1])
        backPadding   = (self.BACK_DISTANCE_TOLERANCE  * paddingScale * -predVectNorm[0],
                         self.BACK_DISTANCE_TOLERANCE  * paddingScale * -predVectNorm[1])
        leftPadding   = (self.SIDE_DISTANCE_TOLERANCE  * paddingScale *  predVect90Norm[0],
                         self.SIDE_DISTANCE_TOLERANCE  * paddingScale *  predVect90Norm[1])
        rightPadding  = (self.SIDE_DISTANCE_TOLERANCE  * paddingScale * -predVect90Norm[0],
                         self.SIDE_DISTANCE_TOLERANCE  * paddingScale * -predVect90Norm[1])

        # generate rectangle points
        frpt = (predpt[0] + frontPadding[0] + rightPadding[0],
                predpt[1] + frontPadding[1] + rightPadding[1])
        flpt = (predpt[0] + frontPadding[0] + leftPadding[0],
                predpt[1] + frontPadding[1] + leftPadding[1])
        brpt = (currpt[0] + backPadding[0]  + rightPadding[0],
                currpt[1] + backPadding[1]  + rightPadding[1])
        blpt = (currpt[0] + backPadding[0]  + leftPadding[0],
                currpt[1] + backPadding[1]  + leftPadding[1])
        
        # create objects
        point = Point(matchpt[0], matchpt[1])
        polygon = Polygon([flpt, frpt, brpt, blpt])

        # check
        contains = polygon.contains(point)

        return contains

    def _isDistanceInsideLimit(self, currdst: int, objectID: int, matchPos: np.ndarray) -&gt; bool:
        &#34;&#34;&#34;Checks if a match between currdst and matchPos is possible / inside a specified area

        Description:
            May use a static or dynamically adjustable method to check if the match Position (matchPos) is within a maximum distance to the current position

        Args:
            currdst (int): distance to match position
            objectID (int): ID of the object being checked
            matchPos (np.ndarray): 2 element array of the x and y coordinate on the image of the objects potential position

        Returns:
            bool: Returns if matchPos is a possible match for the object at ID objectID
        &#34;&#34;&#34;             
        if self.USE_BETTER_RECTS:
            # get objects predicted next position
            predpt = self._continueMovement(objectID)
            # get current objects position
            currpt = self.objects[objectID]
            # check if matchPos is inside the possible movement area
            inRect = self._isInsideRect(currpt, predpt, matchPos, objectID)

            return inRect
        else:
            # check horizontal and vertical distance
            # worse performance for wide camera angles
            if (currdst &lt; self.DISTANCE_TOLERANCE) and (
                abs(self.objects[objectID][1] - matchPos[1]) &lt; self.VERTICAL_TOLERANCE
            ):
                return True
            return False

    ###################################
    ## UPDATE
    ###################################

    def update(
        self, det: Type[torch.Tensor], img: Type[np.ndarray]
    ) -&gt; np.ndarray:  # det: list of koordinates x,y , x,y, ...
        &#34;&#34;&#34;Updates the internal states of the Centroid tracker.

        Description:
            This function should be called every frame.

            Centroids will be premoved automatically when a detections was not possible.

            Centroids will be removed after a specified amount of frames when no longer detected.


        Args:
            det (torch.Tensor): Detections on (,6) tensor [xyxy, conf, cls]
            img (Type[np.ndarray]): Image in BGR [c, w, h] (not needed for Centroid)

        Returns:
            np.ndarray: Tracked entities in [x1, y1, x2, y2, centre x, centre y, id, cls id]
        &#34;&#34;&#34;        
        # keep track of time for debugging
        update_time_start = int(round(time.time() * 1000))
        # get inputs
        bbox_xyxy, conf, cls = self.transform_detections(det)

        # check if no detections are present
        if len(bbox_xyxy) == 0:
            # loop over any existing tracked objects and mark them as disappeared
            for objectID in list(self.disappeared.keys()):
                # increase disappeared count
                self.disappeared[objectID] += 1
                # continue movement
                self.objects[objectID] = self._continueMovement(objectID)
                # deregister if lifetime is surpassed or object drove away
                if self.disappeared[
                    objectID
                ] &gt; self.MAX_DISAPPEARED or self._isInsideDeregistrationZone(
                    self.objects[objectID]
                ):
                    self._deregister(objectID)

            # return early
            return self._return()

        # initialize an array for the inputs
        inputCentroids = np.zeros((len(bbox_xyxy), 2), dtype=&#34;int&#34;)
        inputBBOX = np.zeros((len(bbox_xyxy), 4), dtype=&#34;int&#34;)
        inputConfidence = np.zeros((len(bbox_xyxy), 1), dtype=&#34;float&#34;)
        inputClass = np.zeros((len(bbox_xyxy), 1), dtype=&#34;int&#34;)
        # loop over the bounding box rectangles
        for (i, bb) in enumerate(bbox_xyxy):
            # use the bounding box coordinates to derive the centroid
            inputCentroids[i] = self._centroid(bb)
            inputBBOX[i] = bb
            inputConfidence[i] = conf[i]
            inputClass[i] = cls[i]

        # if currently no objects are being tracked
        if len(self.objects) == 0:
            for i in range(len(inputCentroids)):
                # only register when not in deregistration zone
                if not self._isInsideDeregistrationZone(inputCentroids[i]):
                    self._register(
                        inputCentroids[i],
                        inputBBOX[i],
                        inputConfidence[i],
                        inputClass[i],
                    )

        # otherwise match existing to detected centroids
        else:
            objectIDs = list(self.objects.keys())
            objectCentroids = list(self.objects.values())
            # compute distances
            D = dist.cdist(np.array(objectCentroids), inputCentroids)
            # create sorted list of tuples of indexes and value in ascending order
            D_sorted = sorted(np.ndenumerate(D), key=itemgetter(1))
            # keep track of used Rows and Cols
            usedRows = set()
            usedCols = set()

            # loop through row,col and distance
            for line in D_sorted:
                row = line[0][0]
                col = line[0][1]
                distance = line[1]
                # if row or col already used, skip
                if row in usedRows or col in usedCols:
                    continue
                # get objectID
                objectID = objectIDs[row]
                # otherwise a match is created but only if inside a maximum distance
                if self._isDistanceInsideLimit(distance, objectID, inputCentroids[col]):
                    # safe the distance travelled
                    self.lastDistTrav[objectID] = self._computeLastDist(
                        self.objects[objectID], inputCentroids[col]
                    )
                    # update object
                    self.objects[objectID] = inputCentroids[col]
                    self.bbox[objectID] = inputBBOX[col]
                    self.confidence[objectID] = inputConfidence[col]
                    self.classIds[objectID] = inputClass[col]
                    self.disappeared[objectID] = 0
                    # remove Row and Col
                    usedRows.add(row)
                    usedCols.add(col)

            # get not matched rows and cols
            unusedRows = set(range(0, D.shape[0])).difference(usedRows)
            unusedCols = set(range(0, D.shape[1])).difference(usedCols)

            # unmatched objects will be counted as disappeared
            for row in unusedRows:
                # get object ID
                objectID = objectIDs[row]
                # increase disappeared count
                self.disappeared[objectID] += 1
                # continue movement
                self.objects[objectID] = self._continueMovement(objectID)
                # deregister if lifetime is surpassed or object drove away
                if self.disappeared[
                    objectID
                ] &gt; self.MAX_DISAPPEARED or self._isInsideDeregistrationZone(
                    objectCentroids[row]
                ):
                    self._deregister(objectID)

            # unmatched input centroids will get registered
            for col in unusedCols:
                if not self._isInsideDeregistrationZone(inputCentroids[col]):
                    if self._isInsideRegistrationZone(inputCentroids[col]):
                        self._register(
                            inputCentroids[col],
                            inputBBOX[col],
                            inputConfidence[col],
                            inputClass[col],
                        )

        # keep track of time for debugging
        update_time_end = int(round(time.time() * 1000))
        self.l.debug(
            &#34;Centroid update took {} ms&#34;.format(update_time_end - update_time_start)
        )

        return self._return()

    @staticmethod
    def transform_detections(det: Type[torch.Tensor]):
        &#34;&#34;&#34;Transform detection vector to numpy.
        Args:
            det (torch.Tensor): Detections on (,6) tensor [xyxy, conf, cls]
        Returns:
            Tuple[np.ndarray, np.ndarray, np.ndarray]:
                 [(x1, y1, x2, y2),
                  (class confidences),
                  (model-specific class indices]
        &#34;&#34;&#34;
        t_det = det.cpu().detach().numpy()
        return t_det[:, :4], t_det[..., 4], t_det[..., 5]</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="pancake.tracker.tracker_centroid.CentroidTracker"><code class="flex name class">
<span>class <span class="ident">CentroidTracker</span></span>
<span>(</span><span>cfg:Â dict, *args, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>[<em>Abstract Class</em>] Base class of the Trackers</p>
<p><strong>Base Class</strong>:
All tracking algorithms to be used within this framework have to inherit from this class.
The inheritance will automatically register every subclass into the registry thus
allowing for modular access to the trackers.</p>
<p>Initializes the CentroidTracker</p>
<h2 id="description">Description</h2>
<p>Sets up the internal variables and allocates the OrderedDicts for further processing.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>cfg</code></strong> :&ensp;<code>dict</code></dt>
<dd>dictionary including all configuration variables.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class CentroidTracker(BaseTracker):
    def __init__(self, cfg: dict, *args, **kwargs) -&gt; None:
        &#34;&#34;&#34;Initializes the CentroidTracker

        Description:
            Sets up the internal variables and allocates the OrderedDicts for further processing.

        Args:
            cfg (dict): dictionary including all configuration variables.
        &#34;&#34;&#34;        
        self.l = setup_logger(__name__)  # You can use any name
        self.l.debug(&#34;INIT CENTROID TRACKER - Final&#34;)
        # options
        self.USE_BETTER_RECTS = cfg.CENTROID.USE_BETTER_RECTS
        self.USE_DYNAMIC_SCALING = cfg.CENTROID.USE_DYNAMIC_SCALING
        # constants
        self.MAX_ID = cfg.CENTROID.MAX_ID
        self.MAX_DISAPPEARED = cfg.CENTROID.MAX_DISAPPEARED
        # car match tolerances
        self.DISTANCE_TOLERANCE = cfg.CENTROID.DISTANCE_TOLERANCE
        self.VERTICAL_TOLERANCE = cfg.CENTROID.VERTICAL_TOLERANCE
        self.FRONT_DISTANCE_TOLERANCE = cfg.CENTROID.FRONT_DISTANCE_TOLERANCE
        self.BACK_DISTANCE_TOLERANCE = cfg.CENTROID.BACK_DISTANCE_TOLERANCE
        self.SIDE_DISTANCE_TOLERANCE = cfg.CENTROID.SIDE_DISTANCE_TOLERANCE
        # image boundaries regions
        self.FRAME_WIDTH = cfg.CENTROID.FRAME_WIDTH
        self.FRAME_CHANGE_LC = self.FRAME_WIDTH // 3
        self.FRAME_CHANGE_CR = self.FRAME_CHANGE_LC * 2
        # image transition region size
        self.TRANSITION_WIDTH = cfg.CENTROID.TRANSITION_WIDTH
        # lane separators
        self.LANE_SEPARATOR_LL = cfg.CENTROID.LANE_SEPARATOR_LL
        self.LANE_SEPARATOR_LC = cfg.CENTROID.LANE_SEPARATOR_LC
        self.LANE_SEPARATOR_CR = cfg.CENTROID.LANE_SEPARATOR_CR
        self.LANE_SEPARATOR_RR = cfg.CENTROID.LANE_SEPARATOR_RR
        # deregistration zone boundaries
        self.DEREG_ZONE_L = cfg.CENTROID.DEREG_ZONE_L
        self.DEREG_ZONE_R = cfg.CENTROID.DEREG_ZONE_R
        # registration zone boundaries
        self.REG_ZONE_L = cfg.CENTROID.REG_ZONE_L
        self.REG_ZONE_R = cfg.CENTROID.REG_ZONE_R
        # initialize the next unique object ID along with two ordered
        # dictionaries used to keep track of mapping a given object
        # ID to its centroid and number of consecutive frames it has
        # been marked as &#34;disappeared&#34;, respectively
        self.nextObjectID = 0
        self.objects = OrderedDict()
        self.disappeared = OrderedDict()
        # dicts for return types
        self.bbox = OrderedDict()
        self.confidence = OrderedDict()
        self.classIds = OrderedDict()
        # known previous distant travelled
        self.lastDistTrav = OrderedDict()

    def _centroid(self, vertices: np.ndarray) -&gt; Tuple[int, int]:
        &#34;&#34;&#34;Calculates a centroid from the BBOX coordinates

        Description:
            May receive an unlimited amount of coordinates. But these coordinates need to alternate between x and y.

        Args:
            vertices (np.ndarray): array of x and y coordinates

        Returns:
            Tuple[int, int]: tuple describing the x and y coordinates on the image
        &#34;&#34;&#34;        
        x_list = [vertex for vertex in vertices[::2]]
        y_list = [vertex for vertex in vertices[1::2]]
        x = int(sum(x_list) // len(x_list))
        y = int(sum(y_list) // len(y_list))
        return x, y

    def _register(self, centroid: np.ndarray, bbox: np.ndarray, conf: np.ndarray, cls: np.ndarray):
        &#34;&#34;&#34;Registers an object

        Args:
            centroid (np.ndarray): 2 element array of the coordinates of the object
            bbox (np.ndarray): 4 element array of the top left and bottom right bounding box limits of the object
            conf (np.ndarray): 1 element array of the confidence of the object
            cls (np.ndarray): 1 element array of the class of the object
        &#34;&#34;&#34;        
        # when registering an object we use the next available object
        # ID to store the centroid
        self.objects[self.nextObjectID] = centroid
        self.disappeared[self.nextObjectID] = 0
        # dont add lastDistTrav
        # add other information
        self.bbox[self.nextObjectID] = bbox
        self.confidence[self.nextObjectID] = conf
        self.classIds[self.nextObjectID] = cls
        # increment objectID
        self.nextObjectID += 1
        if self.nextObjectID &gt;= self.MAX_ID:
            self.nextObjectID = 0

    def _deregister(self, objectID: int):
        &#34;&#34;&#34;Deletes an object by ID

        Args:
            objectID (int): ID of the object to be deleted
        &#34;&#34;&#34;        
        # to deregister an object ID we delete the object ID from
        # all of our respective dictionaries
        del self.objects[objectID]
        del self.disappeared[objectID]
        del self.bbox[objectID]
        del self.confidence[objectID]
        del self.classIds[objectID]
        try:
            del self.lastDistTrav[objectID]
        except Exception:
            pass

    def _return(self) -&gt; np.ndarray:
        &#34;&#34;&#34;Returns the Centroids with their respective data as required by the update function.

        Returns:
            np.ndarray: Tracked entities in [x1, y1, x2, y2, centre x, centre y, id, cls id]
        &#34;&#34;&#34;        
        outputs = []
        for id in list(self.objects.keys()):
            outputs.append(
                np.array(
                    [
                        self.bbox[id][0],
                        self.bbox[id][1],
                        self.bbox[id][2],
                        self.bbox[id][3],
                        self.objects[id][0],
                        self.objects[id][1],
                        id,
                        self.classIds[id],
                    ],
                    dtype=np.int,
                )
            )
        # output np array
        if len(outputs) &gt; 0:
            outputs = np.stack(outputs, axis=0)
        return outputs

    def _computeLastDist(self, prevPos: np.ndarray, currPos: np.ndarray) -&gt; Tuple[int,int]:
        &#34;&#34;&#34;Computes the distance vector between two points

        Description:
            Used as a measure between the last known position and the predicted next position

        Args:
            prevPos (np.ndarray): 2 element array of the x and y coordinates in the image of the previously known position (first position)
            currPos (np.ndarray): 2 element array of the x and y coordinates in the image of the currently known position (second position)

        Returns:
            Tuple[int,int]: 2 element tuple of the x and y coordinates in the image of the vector from the previous to the current position
        &#34;&#34;&#34;        
        return currPos[0] - prevPos[0], currPos[1] - prevPos[1]

    def _isAboveLaneSeparator(self, centroid: np.ndarray) -&gt; bool:
        &#34;&#34;&#34;Checks if a centroid is on the upper or lower lane on the image

        Args:
            centroid (np.ndarray): 2 element array of the x and y coordinates on the image of the checked position

        Returns:
            bool: Returns if the input centroid is on the upper (True) or lower (False) driving lane
        &#34;&#34;&#34;        
        if centroid[0] &lt; self.FRAME_CHANGE_LC:
            # left image
            lanesepvec = (
                self.FRAME_CHANGE_LC - 0,
                self.LANE_SEPARATOR_LC - self.LANE_SEPARATOR_LL,
            )
            centrvec = (
                self.FRAME_CHANGE_LC - centroid[0],
                self.LANE_SEPARATOR_LC - centroid[1],
            )
        elif centroid[0] &gt; self.FRAME_CHANGE_CR:
            # right image
            lanesepvec = (
                self.FRAME_WIDTH - self.FRAME_CHANGE_CR,
                self.LANE_SEPARATOR_RR - self.LANE_SEPARATOR_CR,
            )
            centrvec = (
                self.FRAME_WIDTH - centroid[0],
                self.LANE_SEPARATOR_RR - centroid[1],
            )
        else:
            # center image
            lanesepvec = (
                self.FRAME_CHANGE_CR - self.FRAME_CHANGE_LC,
                self.LANE_SEPARATOR_CR - self.LANE_SEPARATOR_LC,
            )
            centrvec = (
                self.FRAME_CHANGE_CR - centroid[0],
                self.LANE_SEPARATOR_CR - centroid[1],
            )
        # calculate cross product in 2d
        cross_product = lanesepvec[0] * centrvec[1] - lanesepvec[1] * centrvec[0]
        return cross_product &gt; 0

    def _isInsideDeregistrationZone(self, centroid: np.ndarray) -&gt; bool:
        &#34;&#34;&#34;Determines whether a centroid is inside the deregistration zone on the left and right cameras edges

        Args:
            centroid (np.ndarray): 2 element array of the x and y coordinates on the image of the position being checked

        Returns:
            bool: Returns whether the input centroid is inside the deregistration zone
        &#34;&#34;&#34;
        if centroid[0] &lt; self.DEREG_ZONE_L:
            # left dereg
            return self._isAboveLaneSeparator(centroid)
        elif centroid[0] &gt; self.DEREG_ZONE_R:
            # right dereg
            return not self._isAboveLaneSeparator(centroid)
        else:
            # between deregs
            return False

    def _isInsideRegistrationZone(self, centroid: np. ndarray) -&gt; bool:
        &#34;&#34;&#34;Determines whether a centroid is inside the registration zone on the left and right cameras edges

        Args:
            centroid (np.ndarray): 2 element array of the x and y coordinates on the image of the position being checked

        Returns:
            bool: Returns whether the input centroid is inside the registration zone
        &#34;&#34;&#34;
        if centroid[0] &lt; self.REG_ZONE_L:
            # left reg
            return not self._isAboveLaneSeparator(centroid)
        elif centroid[0] &gt; self.REG_ZONE_R:
            # right reg
            return self._isAboveLaneSeparator(centroid)
        else:
            # between regs
            return False

    def _isInsideTransitionZone(self, centroid: np.ndarray) -&gt; bool:
        &#34;&#34;&#34;Determines whether a centroid is inside the transition region between camera images

        Args:
            centroid (np.ndarray): 2 element array of the x and y coordinates on the image of the position being checked

        Returns:
            bool: Returns whether the input centroid is inside the transition region
        &#34;&#34;&#34;        
        if (
            centroid[0] &lt; self.FRAME_CHANGE_LC + self.TRANSITION_WIDTH
            and centroid[0] &gt; self.FRAME_CHANGE_LC - self.TRANSITION_WIDTH
        ):
            # left transition region
            return True
        elif (
            centroid[0] &lt; self.FRAME_CHANGE_CR + self.TRANSITION_WIDTH
            and centroid[0] &gt; self.FRAME_CHANGE_CR - self.TRANSITION_WIDTH
        ):
            # right transition region
            return True
        else:
            # not in transition regions
            return False

    def _getLen(self, tup: Tuple[int, int]) -&gt; float:
        &#34;&#34;&#34;Gets the length of a vector depicted as a tuple

        Args:
            tup (Tuple[int, int]): 2 element tuple describing a vector

        Returns:
            float: length of the vector
        &#34;&#34;&#34;        
        return (tup[0] ** 2 + tup[1] ** 2) ** 0.5

    def _continueMovement(self, objectID: int) -&gt; Tuple[int, int]:
        &#34;&#34;&#34;Continues the movement of a specific object.

        Description:
            Moves an object - depicted by its objectID - to a position where it is predicted to appear in the future.
            This is done by using the last know movement.

        Args:
            objectID (int): ID of the object that should be moved.

        Returns:
            Tuple[int, int]: 2 element tuple of the new predicted position
        &#34;&#34;&#34;        
        objPos = self.objects[objectID]

        try:
            distance = self.lastDistTrav[objectID]
        except Exception:  # object was only seen one frame
            distance = (0, 0)

        if self._isInsideTransitionZone(objPos):
            if objPos[0] &lt; self.FRAME_CHANGE_LC:
                # follow left vector
                dirVect = (
                    0 - self.FRAME_CHANGE_LC,
                    self.LANE_SEPARATOR_LL - self.LANE_SEPARATOR_LC,
                )
            elif objPos[0] &gt; self.FRAME_CHANGE_CR:
                # follow right vector
                dirVect = (
                    self.FRAME_CHANGE_CR - self.FRAME_WIDTH,
                    self.LANE_SEPARATOR_CR - self.LANE_SEPARATOR_RR,
                )
            else:
                # follow center vector
                dirVect = (
                    self.FRAME_CHANGE_LC - self.FRAME_CHANGE_CR,
                    self.LANE_SEPARATOR_LC - self.LANE_SEPARATOR_CR,
                )

            # flip direction of dirVect for bottom lane
            if not self._isAboveLaneSeparator(objPos):
                dirVect = (-dirVect[0], -dirVect[1])

            # change distance according to dirVect
            lenDistance = self._getLen(distance)
            dirVectLen = self._getLen(dirVect)
            dirVectNorm = (dirVect[0] / dirVectLen, dirVect[1] / dirVectLen)
            distance = (dirVectNorm[0] * lenDistance, dirVectNorm[1] * lenDistance)

        predmove = objPos[0] + distance[0], objPos[1] + distance[1]
        return predmove

    def _isInsideRect(self, currpt: np.ndarray, predpt: Tuple[int, int], matchpt: np.ndarray, objID: int) -&gt; bool:
        &#34;&#34;&#34;Checks if matchpt is inside a rectangle

        Description:
            The rectangle is generated by the current position (currpt) and the predicted position (predpt) with some dynamically or statically adjustable padding

        Args:
            currpt (np.ndarray): 2 element array of the x and y coordinates on the image of the current position
            predpt (Tuple[int, int]): 2 element tuple of the x and y coordinates on the image of the predicted position
            matchpt (np.ndarray): 2 element array of the x and y coordinates on the image of the potential match
            objID (int): ID of the object currently inspected

        Returns:
            bool: Returns if matchpt is inside a rectangle
        &#34;&#34;&#34;        
        if self.USE_DYNAMIC_SCALING:
            # get scaling based on camera
            if currpt[0] &lt; self.FRAME_CHANGE_LC:
                # left camera
                objectSize = 0.0863384 * currpt[0] - 109.93
            elif currpt[0] &gt; self.FRAME_CHANGE_CR:
                # right camera
                objectSize = -0.060652 * currpt[0] + 676.94
            else:
                # center camera
                objectSize = 228
            # normalize scale to center camera
            scale = objectSize / 228
        else:
            scale = 1

        # set scaling
        vectorScale = scale
        paddingScale = scale

        # get vector from currpt to predpt
        predVect = (predpt[0] - currpt[0], predpt[1] - currpt[1])

        # set own prediction vector if no prediction was made previously
        if predVect == (0,0):
            if currpt[0] &lt; self.FRAME_CHANGE_LC:
                # follow left vector
                predVectTmp = (
                    0 - self.FRAME_CHANGE_LC,
                    self.LANE_SEPARATOR_LL - self.LANE_SEPARATOR_LC,
                )
            elif currpt[0] &gt; self.FRAME_CHANGE_CR:
                # follow right vector
                predVectTmp = (
                    self.FRAME_CHANGE_CR - self.FRAME_WIDTH,
                    self.LANE_SEPARATOR_CR - self.LANE_SEPARATOR_RR,
                )
            else:
                # follow center vector
                predVectTmp = (
                    self.FRAME_CHANGE_LC - self.FRAME_CHANGE_CR,
                    self.LANE_SEPARATOR_LC - self.LANE_SEPARATOR_CR,
                )

            # flip direction of predVectTmp for bottom lane
            if not self._isAboveLaneSeparator(currpt):
                predVectTmp = (-predVectTmp[0], -predVectTmp[1])

            # normalize predVectTmp
            predVectLen = self._getLen(predVectTmp)
            predVectNorm = (predVectTmp[0] / predVectLen, predVectTmp[1] / predVectLen)

            # generate predVect
            predVect = (predVectNorm[0] * vectorScale, predVectNorm[1] * vectorScale)

        # calculate prediction vectors to generate the rectangle from
        predVectLen    = self._getLen(predVect)
        predVectNorm   = (predVect[0] / predVectLen, predVect[1] / predVectLen)
        predVect90Norm = (-predVectNorm[1], predVectNorm[0])

        # calculate padding vectors
        frontPadding  = (self.FRONT_DISTANCE_TOLERANCE * paddingScale *  predVectNorm[0],
                         self.FRONT_DISTANCE_TOLERANCE * paddingScale *  predVectNorm[1])
        backPadding   = (self.BACK_DISTANCE_TOLERANCE  * paddingScale * -predVectNorm[0],
                         self.BACK_DISTANCE_TOLERANCE  * paddingScale * -predVectNorm[1])
        leftPadding   = (self.SIDE_DISTANCE_TOLERANCE  * paddingScale *  predVect90Norm[0],
                         self.SIDE_DISTANCE_TOLERANCE  * paddingScale *  predVect90Norm[1])
        rightPadding  = (self.SIDE_DISTANCE_TOLERANCE  * paddingScale * -predVect90Norm[0],
                         self.SIDE_DISTANCE_TOLERANCE  * paddingScale * -predVect90Norm[1])

        # generate rectangle points
        frpt = (predpt[0] + frontPadding[0] + rightPadding[0],
                predpt[1] + frontPadding[1] + rightPadding[1])
        flpt = (predpt[0] + frontPadding[0] + leftPadding[0],
                predpt[1] + frontPadding[1] + leftPadding[1])
        brpt = (currpt[0] + backPadding[0]  + rightPadding[0],
                currpt[1] + backPadding[1]  + rightPadding[1])
        blpt = (currpt[0] + backPadding[0]  + leftPadding[0],
                currpt[1] + backPadding[1]  + leftPadding[1])
        
        # create objects
        point = Point(matchpt[0], matchpt[1])
        polygon = Polygon([flpt, frpt, brpt, blpt])

        # check
        contains = polygon.contains(point)

        return contains

    def _isDistanceInsideLimit(self, currdst: int, objectID: int, matchPos: np.ndarray) -&gt; bool:
        &#34;&#34;&#34;Checks if a match between currdst and matchPos is possible / inside a specified area

        Description:
            May use a static or dynamically adjustable method to check if the match Position (matchPos) is within a maximum distance to the current position

        Args:
            currdst (int): distance to match position
            objectID (int): ID of the object being checked
            matchPos (np.ndarray): 2 element array of the x and y coordinate on the image of the objects potential position

        Returns:
            bool: Returns if matchPos is a possible match for the object at ID objectID
        &#34;&#34;&#34;             
        if self.USE_BETTER_RECTS:
            # get objects predicted next position
            predpt = self._continueMovement(objectID)
            # get current objects position
            currpt = self.objects[objectID]
            # check if matchPos is inside the possible movement area
            inRect = self._isInsideRect(currpt, predpt, matchPos, objectID)

            return inRect
        else:
            # check horizontal and vertical distance
            # worse performance for wide camera angles
            if (currdst &lt; self.DISTANCE_TOLERANCE) and (
                abs(self.objects[objectID][1] - matchPos[1]) &lt; self.VERTICAL_TOLERANCE
            ):
                return True
            return False

    ###################################
    ## UPDATE
    ###################################

    def update(
        self, det: Type[torch.Tensor], img: Type[np.ndarray]
    ) -&gt; np.ndarray:  # det: list of koordinates x,y , x,y, ...
        &#34;&#34;&#34;Updates the internal states of the Centroid tracker.

        Description:
            This function should be called every frame.

            Centroids will be premoved automatically when a detections was not possible.

            Centroids will be removed after a specified amount of frames when no longer detected.


        Args:
            det (torch.Tensor): Detections on (,6) tensor [xyxy, conf, cls]
            img (Type[np.ndarray]): Image in BGR [c, w, h] (not needed for Centroid)

        Returns:
            np.ndarray: Tracked entities in [x1, y1, x2, y2, centre x, centre y, id, cls id]
        &#34;&#34;&#34;        
        # keep track of time for debugging
        update_time_start = int(round(time.time() * 1000))
        # get inputs
        bbox_xyxy, conf, cls = self.transform_detections(det)

        # check if no detections are present
        if len(bbox_xyxy) == 0:
            # loop over any existing tracked objects and mark them as disappeared
            for objectID in list(self.disappeared.keys()):
                # increase disappeared count
                self.disappeared[objectID] += 1
                # continue movement
                self.objects[objectID] = self._continueMovement(objectID)
                # deregister if lifetime is surpassed or object drove away
                if self.disappeared[
                    objectID
                ] &gt; self.MAX_DISAPPEARED or self._isInsideDeregistrationZone(
                    self.objects[objectID]
                ):
                    self._deregister(objectID)

            # return early
            return self._return()

        # initialize an array for the inputs
        inputCentroids = np.zeros((len(bbox_xyxy), 2), dtype=&#34;int&#34;)
        inputBBOX = np.zeros((len(bbox_xyxy), 4), dtype=&#34;int&#34;)
        inputConfidence = np.zeros((len(bbox_xyxy), 1), dtype=&#34;float&#34;)
        inputClass = np.zeros((len(bbox_xyxy), 1), dtype=&#34;int&#34;)
        # loop over the bounding box rectangles
        for (i, bb) in enumerate(bbox_xyxy):
            # use the bounding box coordinates to derive the centroid
            inputCentroids[i] = self._centroid(bb)
            inputBBOX[i] = bb
            inputConfidence[i] = conf[i]
            inputClass[i] = cls[i]

        # if currently no objects are being tracked
        if len(self.objects) == 0:
            for i in range(len(inputCentroids)):
                # only register when not in deregistration zone
                if not self._isInsideDeregistrationZone(inputCentroids[i]):
                    self._register(
                        inputCentroids[i],
                        inputBBOX[i],
                        inputConfidence[i],
                        inputClass[i],
                    )

        # otherwise match existing to detected centroids
        else:
            objectIDs = list(self.objects.keys())
            objectCentroids = list(self.objects.values())
            # compute distances
            D = dist.cdist(np.array(objectCentroids), inputCentroids)
            # create sorted list of tuples of indexes and value in ascending order
            D_sorted = sorted(np.ndenumerate(D), key=itemgetter(1))
            # keep track of used Rows and Cols
            usedRows = set()
            usedCols = set()

            # loop through row,col and distance
            for line in D_sorted:
                row = line[0][0]
                col = line[0][1]
                distance = line[1]
                # if row or col already used, skip
                if row in usedRows or col in usedCols:
                    continue
                # get objectID
                objectID = objectIDs[row]
                # otherwise a match is created but only if inside a maximum distance
                if self._isDistanceInsideLimit(distance, objectID, inputCentroids[col]):
                    # safe the distance travelled
                    self.lastDistTrav[objectID] = self._computeLastDist(
                        self.objects[objectID], inputCentroids[col]
                    )
                    # update object
                    self.objects[objectID] = inputCentroids[col]
                    self.bbox[objectID] = inputBBOX[col]
                    self.confidence[objectID] = inputConfidence[col]
                    self.classIds[objectID] = inputClass[col]
                    self.disappeared[objectID] = 0
                    # remove Row and Col
                    usedRows.add(row)
                    usedCols.add(col)

            # get not matched rows and cols
            unusedRows = set(range(0, D.shape[0])).difference(usedRows)
            unusedCols = set(range(0, D.shape[1])).difference(usedCols)

            # unmatched objects will be counted as disappeared
            for row in unusedRows:
                # get object ID
                objectID = objectIDs[row]
                # increase disappeared count
                self.disappeared[objectID] += 1
                # continue movement
                self.objects[objectID] = self._continueMovement(objectID)
                # deregister if lifetime is surpassed or object drove away
                if self.disappeared[
                    objectID
                ] &gt; self.MAX_DISAPPEARED or self._isInsideDeregistrationZone(
                    objectCentroids[row]
                ):
                    self._deregister(objectID)

            # unmatched input centroids will get registered
            for col in unusedCols:
                if not self._isInsideDeregistrationZone(inputCentroids[col]):
                    if self._isInsideRegistrationZone(inputCentroids[col]):
                        self._register(
                            inputCentroids[col],
                            inputBBOX[col],
                            inputConfidence[col],
                            inputClass[col],
                        )

        # keep track of time for debugging
        update_time_end = int(round(time.time() * 1000))
        self.l.debug(
            &#34;Centroid update took {} ms&#34;.format(update_time_end - update_time_start)
        )

        return self._return()

    @staticmethod
    def transform_detections(det: Type[torch.Tensor]):
        &#34;&#34;&#34;Transform detection vector to numpy.
        Args:
            det (torch.Tensor): Detections on (,6) tensor [xyxy, conf, cls]
        Returns:
            Tuple[np.ndarray, np.ndarray, np.ndarray]:
                 [(x1, y1, x2, y2),
                  (class confidences),
                  (model-specific class indices]
        &#34;&#34;&#34;
        t_det = det.cpu().detach().numpy()
        return t_det[:, :4], t_det[..., 4], t_det[..., 5]</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="pancake.tracker.tracker.BaseTracker" href="tracker.html#pancake.tracker.tracker.BaseTracker">BaseTracker</a></li>
<li>abc.ABC</li>
</ul>
<h3>Static methods</h3>
<dl>
<dt id="pancake.tracker.tracker_centroid.CentroidTracker.transform_detections"><code class="name flex">
<span>def <span class="ident">transform_detections</span></span>(<span>det:Â Type[torch.Tensor])</span>
</code></dt>
<dd>
<div class="desc"><p>Transform detection vector to numpy.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>det</code></strong> :&ensp;<code>torch.Tensor</code></dt>
<dd>Detections on (,6) tensor [xyxy, conf, cls]</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Tuple[np.ndarray, np.ndarray, np.ndarray]:
[(x1, y1, x2, y2),
(class confidences),
(model-specific class indices]</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def transform_detections(det: Type[torch.Tensor]):
    &#34;&#34;&#34;Transform detection vector to numpy.
    Args:
        det (torch.Tensor): Detections on (,6) tensor [xyxy, conf, cls]
    Returns:
        Tuple[np.ndarray, np.ndarray, np.ndarray]:
             [(x1, y1, x2, y2),
              (class confidences),
              (model-specific class indices]
    &#34;&#34;&#34;
    t_det = det.cpu().detach().numpy()
    return t_det[:, :4], t_det[..., 4], t_det[..., 5]</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="pancake.tracker.tracker_centroid.CentroidTracker.update"><code class="name flex">
<span>def <span class="ident">update</span></span>(<span>self, det:Â Type[torch.Tensor], img:Â Type[numpy.ndarray]) â>Â numpy.ndarray</span>
</code></dt>
<dd>
<div class="desc"><p>Updates the internal states of the Centroid tracker.</p>
<h2 id="description">Description</h2>
<p>This function should be called every frame.</p>
<p>Centroids will be premoved automatically when a detections was not possible.</p>
<p>Centroids will be removed after a specified amount of frames when no longer detected.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>det</code></strong> :&ensp;<code>torch.Tensor</code></dt>
<dd>Detections on (,6) tensor [xyxy, conf, cls]</dd>
<dt><strong><code>img</code></strong> :&ensp;<code>Type[np.ndarray]</code></dt>
<dd>Image in BGR [c, w, h] (not needed for Centroid)</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>np.ndarray</code></dt>
<dd>Tracked entities in [x1, y1, x2, y2, centre x, centre y, id, cls id]</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def update(
    self, det: Type[torch.Tensor], img: Type[np.ndarray]
) -&gt; np.ndarray:  # det: list of koordinates x,y , x,y, ...
    &#34;&#34;&#34;Updates the internal states of the Centroid tracker.

    Description:
        This function should be called every frame.

        Centroids will be premoved automatically when a detections was not possible.

        Centroids will be removed after a specified amount of frames when no longer detected.


    Args:
        det (torch.Tensor): Detections on (,6) tensor [xyxy, conf, cls]
        img (Type[np.ndarray]): Image in BGR [c, w, h] (not needed for Centroid)

    Returns:
        np.ndarray: Tracked entities in [x1, y1, x2, y2, centre x, centre y, id, cls id]
    &#34;&#34;&#34;        
    # keep track of time for debugging
    update_time_start = int(round(time.time() * 1000))
    # get inputs
    bbox_xyxy, conf, cls = self.transform_detections(det)

    # check if no detections are present
    if len(bbox_xyxy) == 0:
        # loop over any existing tracked objects and mark them as disappeared
        for objectID in list(self.disappeared.keys()):
            # increase disappeared count
            self.disappeared[objectID] += 1
            # continue movement
            self.objects[objectID] = self._continueMovement(objectID)
            # deregister if lifetime is surpassed or object drove away
            if self.disappeared[
                objectID
            ] &gt; self.MAX_DISAPPEARED or self._isInsideDeregistrationZone(
                self.objects[objectID]
            ):
                self._deregister(objectID)

        # return early
        return self._return()

    # initialize an array for the inputs
    inputCentroids = np.zeros((len(bbox_xyxy), 2), dtype=&#34;int&#34;)
    inputBBOX = np.zeros((len(bbox_xyxy), 4), dtype=&#34;int&#34;)
    inputConfidence = np.zeros((len(bbox_xyxy), 1), dtype=&#34;float&#34;)
    inputClass = np.zeros((len(bbox_xyxy), 1), dtype=&#34;int&#34;)
    # loop over the bounding box rectangles
    for (i, bb) in enumerate(bbox_xyxy):
        # use the bounding box coordinates to derive the centroid
        inputCentroids[i] = self._centroid(bb)
        inputBBOX[i] = bb
        inputConfidence[i] = conf[i]
        inputClass[i] = cls[i]

    # if currently no objects are being tracked
    if len(self.objects) == 0:
        for i in range(len(inputCentroids)):
            # only register when not in deregistration zone
            if not self._isInsideDeregistrationZone(inputCentroids[i]):
                self._register(
                    inputCentroids[i],
                    inputBBOX[i],
                    inputConfidence[i],
                    inputClass[i],
                )

    # otherwise match existing to detected centroids
    else:
        objectIDs = list(self.objects.keys())
        objectCentroids = list(self.objects.values())
        # compute distances
        D = dist.cdist(np.array(objectCentroids), inputCentroids)
        # create sorted list of tuples of indexes and value in ascending order
        D_sorted = sorted(np.ndenumerate(D), key=itemgetter(1))
        # keep track of used Rows and Cols
        usedRows = set()
        usedCols = set()

        # loop through row,col and distance
        for line in D_sorted:
            row = line[0][0]
            col = line[0][1]
            distance = line[1]
            # if row or col already used, skip
            if row in usedRows or col in usedCols:
                continue
            # get objectID
            objectID = objectIDs[row]
            # otherwise a match is created but only if inside a maximum distance
            if self._isDistanceInsideLimit(distance, objectID, inputCentroids[col]):
                # safe the distance travelled
                self.lastDistTrav[objectID] = self._computeLastDist(
                    self.objects[objectID], inputCentroids[col]
                )
                # update object
                self.objects[objectID] = inputCentroids[col]
                self.bbox[objectID] = inputBBOX[col]
                self.confidence[objectID] = inputConfidence[col]
                self.classIds[objectID] = inputClass[col]
                self.disappeared[objectID] = 0
                # remove Row and Col
                usedRows.add(row)
                usedCols.add(col)

        # get not matched rows and cols
        unusedRows = set(range(0, D.shape[0])).difference(usedRows)
        unusedCols = set(range(0, D.shape[1])).difference(usedCols)

        # unmatched objects will be counted as disappeared
        for row in unusedRows:
            # get object ID
            objectID = objectIDs[row]
            # increase disappeared count
            self.disappeared[objectID] += 1
            # continue movement
            self.objects[objectID] = self._continueMovement(objectID)
            # deregister if lifetime is surpassed or object drove away
            if self.disappeared[
                objectID
            ] &gt; self.MAX_DISAPPEARED or self._isInsideDeregistrationZone(
                objectCentroids[row]
            ):
                self._deregister(objectID)

        # unmatched input centroids will get registered
        for col in unusedCols:
            if not self._isInsideDeregistrationZone(inputCentroids[col]):
                if self._isInsideRegistrationZone(inputCentroids[col]):
                    self._register(
                        inputCentroids[col],
                        inputBBOX[col],
                        inputConfidence[col],
                        inputClass[col],
                    )

    # keep track of time for debugging
    update_time_end = int(round(time.time() * 1000))
    self.l.debug(
        &#34;Centroid update took {} ms&#34;.format(update_time_end - update_time_start)
    )

    return self._return()</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="pancake.tracker.tracker.BaseTracker" href="tracker.html#pancake.tracker.tracker.BaseTracker">BaseTracker</a></b></code>:
<ul class="hlist">
<li><code><a title="pancake.tracker.tracker.BaseTracker.get_subclasses" href="tracker.html#pancake.tracker.tracker.BaseTracker.get_subclasses">get_subclasses</a></code></li>
</ul>
</li>
</ul>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="pancake.tracker" href="index.html">pancake.tracker</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="pancake.tracker.tracker_centroid.CentroidTracker" href="#pancake.tracker.tracker_centroid.CentroidTracker">CentroidTracker</a></code></h4>
<ul class="">
<li><code><a title="pancake.tracker.tracker_centroid.CentroidTracker.transform_detections" href="#pancake.tracker.tracker_centroid.CentroidTracker.transform_detections">transform_detections</a></code></li>
<li><code><a title="pancake.tracker.tracker_centroid.CentroidTracker.update" href="#pancake.tracker.tracker_centroid.CentroidTracker.update">update</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.9.2</a>.</p>
</footer>
</body>
</html>